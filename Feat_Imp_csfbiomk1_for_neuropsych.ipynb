{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af7ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c94958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling biomarkers\n",
    "bloodbiomk1_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\bloodbiomk1_20231117.csv\"\n",
    "bloodbiomk1 = pd.read_csv(bloodbiomk1_file_path)\n",
    "bloodbiomk1 = pd.DataFrame(bloodbiomk1)\n",
    "\n",
    "bloodbiomk2_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\bloodbiomk2_20231117.csv\"\n",
    "bloodbiomk2 = pd.read_csv(bloodbiomk2_file_path)\n",
    "bloodbiomk2 = pd.DataFrame(bloodbiomk2)\n",
    "\n",
    "csfbiomk1_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk1_20231117.csv\"\n",
    "csfbiomk1 = pd.read_csv(csfbiomk1_file_path)\n",
    "csfbiomk1 = pd.DataFrame(csfbiomk1)\n",
    "\n",
    "csfbiomk2_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk2_20231117.csv\"\n",
    "csfbiomk2 = pd.read_csv(csfbiomk2_file_path)\n",
    "csfbiomk2 = pd.DataFrame(csfbiomk2)\n",
    "\n",
    "csfbiomk3_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk3_20231117.csv\"\n",
    "csfbiomk3 = pd.read_csv(csfbiomk3_file_path)\n",
    "csfbiomk3 = pd.DataFrame(csfbiomk3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae0e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main_path = r'Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers'\n",
    "folder_name = 'csf_biomk1_to_neuropsych'  \n",
    "save_path = os.path.join(main_path, folder_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# save_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\blood_biomk1_neuropsych\" # Update this path to your desired folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c104242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id  visit  checkin_bin  exposurebin  age_decade  racecat_combined  \\\n",
      "0      1001.0      1            2            1         1.0               5.0   \n",
      "1      1002.0      1            2            1         1.0               5.0   \n",
      "2      1003.0      1            2            1         1.0               5.0   \n",
      "3      1004.0      1            1            1         2.0               5.0   \n",
      "4      1005.0      1            3            0         2.0               5.0   \n",
      "\n",
      "   eduyears  totyr_foot  chiiseas_pf  chiiyrs_pf  ...    c_pT181  \\\n",
      "0      16.0         7.0       4335.4      2167.7  ...  24.400000   \n",
      "1      15.0        14.0      10363.1      5708.1  ...  32.220000   \n",
      "2      18.0        12.0       6685.4      4863.9  ...  32.220000   \n",
      "3      16.0        16.0       7701.2      6448.9  ...  39.058000   \n",
      "4      21.0         NaN          NaN         NaN  ...  38.311628   \n",
      "\n",
      "   c_tTau_FLAG_below_ref_range  c_FLAG_hemolysis  c_tTau_FLAG_insufficient  \\\n",
      "0                     0.000000          0.000000                      0.00   \n",
      "1                     0.075000          0.025000                      0.00   \n",
      "2                     0.075000          0.025000                      0.00   \n",
      "3                     0.060000          0.010000                      0.01   \n",
      "4                     0.093023          0.023256                      0.00   \n",
      "\n",
      "       c_ttau       c_NfL       c_GFAP     c_pT231   c_pT217  \\\n",
      "0  598.000000  373.960000  8706.690000  487.340000  0.605900   \n",
      "1  401.702703  735.907250  8163.845946  501.851892  1.254549   \n",
      "2  401.702703  735.907250  8163.845946  501.851892  1.254549   \n",
      "3  418.709677  780.569900  9070.057586  614.922874  2.080534   \n",
      "4  393.948718  756.707209  8165.744286  525.304762  1.457107   \n",
      "\n",
      "   dxcte_ptau217_csf_flag_dv  \n",
      "0                   1.000000  \n",
      "1                   1.100000  \n",
      "2                   1.100000  \n",
      "3                   1.175258  \n",
      "4                   1.093023  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean for each column, ignoring NaNs\n",
    "# Replace nulls with label means\n",
    "\n",
    "for column in csfbiomk1.columns:\n",
    "    if column != 'checkin_bin':  # Skip the label column\n",
    "\n",
    "        averages = csfbiomk1.groupby('checkin_bin')[column].mean()\n",
    "        \n",
    "        # Replace nulls for each label separately\n",
    "        for label in averages.index:\n",
    "            csfbiomk1.loc[(csfbiomk1['checkin_bin'] == label) & (csfbiomk1[column].isnull()), column] = averages[label]\n",
    "\n",
    "print(csfbiomk1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f107621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csfbiomk1.to_csv(r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk1_nullreplaced.csv\")\n",
    "original_data = csfbiomk1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fa9593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236 entries, 0 to 235\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   c_strem2     236 non-null    float64\n",
      " 1   c_PDGFRbeta  236 non-null    float64\n",
      " 2   c_Ab40       236 non-null    float64\n",
      " 3   c_Ab42       236 non-null    float64\n",
      " 4   c_pT181      236 non-null    float64\n",
      " 5   c_ttau       236 non-null    float64\n",
      " 6   c_NfL        236 non-null    float64\n",
      " 7   c_GFAP       236 non-null    float64\n",
      " 8   c_pT231      236 non-null    float64\n",
      " 9   c_pT217      236 non-null    float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 18.6 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_strem2</th>\n",
       "      <th>c_PDGFRbeta</th>\n",
       "      <th>c_Ab40</th>\n",
       "      <th>c_Ab42</th>\n",
       "      <th>c_pT181</th>\n",
       "      <th>c_ttau</th>\n",
       "      <th>c_NfL</th>\n",
       "      <th>c_GFAP</th>\n",
       "      <th>c_pT231</th>\n",
       "      <th>c_pT217</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2035.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>8046.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>598.000000</td>\n",
       "      <td>373.960000</td>\n",
       "      <td>8706.690000</td>\n",
       "      <td>487.340000</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2397.612245</td>\n",
       "      <td>361.773196</td>\n",
       "      <td>9599.500000</td>\n",
       "      <td>790.090000</td>\n",
       "      <td>39.058000</td>\n",
       "      <td>418.709677</td>\n",
       "      <td>780.569900</td>\n",
       "      <td>9070.057586</td>\n",
       "      <td>614.922874</td>\n",
       "      <td>2.080534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2752.976744</td>\n",
       "      <td>410.348837</td>\n",
       "      <td>11663.651163</td>\n",
       "      <td>964.255814</td>\n",
       "      <td>38.311628</td>\n",
       "      <td>393.948718</td>\n",
       "      <td>756.707209</td>\n",
       "      <td>8165.744286</td>\n",
       "      <td>525.304762</td>\n",
       "      <td>1.457107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      c_strem2  c_PDGFRbeta        c_Ab40      c_Ab42    c_pT181      c_ttau  \\\n",
       "0  2035.000000   295.000000   8046.000000  702.000000  24.400000  598.000000   \n",
       "1  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
       "2  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
       "3  2397.612245   361.773196   9599.500000  790.090000  39.058000  418.709677   \n",
       "4  2752.976744   410.348837  11663.651163  964.255814  38.311628  393.948718   \n",
       "\n",
       "        c_NfL       c_GFAP     c_pT231   c_pT217  \n",
       "0  373.960000  8706.690000  487.340000  0.605900  \n",
       "1  735.907250  8163.845946  501.851892  1.254549  \n",
       "2  735.907250  8163.845946  501.851892  1.254549  \n",
       "3  780.569900  9070.057586  614.922874  2.080534  \n",
       "4  756.707209  8165.744286  525.304762  1.457107  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csfbiomk1 = csfbiomk1.drop(columns=csfbiomk1.loc[:,'subject_id':'chiiyrs_pg'].columns)\n",
    "csfbiomk1 = csfbiomk1.drop(columns='c_strem2_FLAG_insufficient')\n",
    "csfbiomk1 = csfbiomk1.drop(columns=['c_PDGFRbeta_FLAG_insufficient',\n",
    "                              'c_tTau_FLAG_below_ref_range',\n",
    "                              'c_FLAG_hemolysis', \n",
    "                              'c_tTau_FLAG_insufficient',\n",
    "                              'dxcte_ptau217_csf_flag_dv'])\n",
    "csfbiomk1.info()\n",
    "csfbiomk1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d355d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_strem2</th>\n",
       "      <th>c_PDGFRbeta</th>\n",
       "      <th>c_Ab40</th>\n",
       "      <th>c_Ab42</th>\n",
       "      <th>c_pT181</th>\n",
       "      <th>c_ttau</th>\n",
       "      <th>c_NfL</th>\n",
       "      <th>c_GFAP</th>\n",
       "      <th>c_pT231</th>\n",
       "      <th>c_pT217</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2035.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>8046.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>598.000000</td>\n",
       "      <td>373.960000</td>\n",
       "      <td>8706.690000</td>\n",
       "      <td>487.340000</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2397.612245</td>\n",
       "      <td>361.773196</td>\n",
       "      <td>9599.500000</td>\n",
       "      <td>790.090000</td>\n",
       "      <td>39.058000</td>\n",
       "      <td>418.709677</td>\n",
       "      <td>780.569900</td>\n",
       "      <td>9070.057586</td>\n",
       "      <td>614.922874</td>\n",
       "      <td>2.080534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2752.976744</td>\n",
       "      <td>410.348837</td>\n",
       "      <td>11663.651163</td>\n",
       "      <td>964.255814</td>\n",
       "      <td>38.311628</td>\n",
       "      <td>393.948718</td>\n",
       "      <td>756.707209</td>\n",
       "      <td>8165.744286</td>\n",
       "      <td>525.304762</td>\n",
       "      <td>1.457107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2397.612245</td>\n",
       "      <td>361.773196</td>\n",
       "      <td>9599.500000</td>\n",
       "      <td>790.090000</td>\n",
       "      <td>39.058000</td>\n",
       "      <td>418.709677</td>\n",
       "      <td>780.569900</td>\n",
       "      <td>9070.057586</td>\n",
       "      <td>614.922874</td>\n",
       "      <td>0.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4626.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>11797.000000</td>\n",
       "      <td>936.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>1075.590000</td>\n",
       "      <td>37506.300000</td>\n",
       "      <td>609.430000</td>\n",
       "      <td>0.481200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2397.612245</td>\n",
       "      <td>361.773196</td>\n",
       "      <td>9599.500000</td>\n",
       "      <td>790.090000</td>\n",
       "      <td>39.058000</td>\n",
       "      <td>418.709677</td>\n",
       "      <td>780.569900</td>\n",
       "      <td>9070.057586</td>\n",
       "      <td>614.922874</td>\n",
       "      <td>2.080534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      c_strem2  c_PDGFRbeta        c_Ab40      c_Ab42    c_pT181      c_ttau  \\\n",
       "0  2035.000000   295.000000   8046.000000  702.000000  24.400000  598.000000   \n",
       "1  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
       "2  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
       "3  2397.612245   361.773196   9599.500000  790.090000  39.058000  418.709677   \n",
       "4  2752.976744   410.348837  11663.651163  964.255814  38.311628  393.948718   \n",
       "5  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
       "6  2397.612245   361.773196   9599.500000  790.090000  39.058000  418.709677   \n",
       "7  4626.000000   391.000000  11797.000000  936.000000  40.000000  310.000000   \n",
       "8  2397.612245   361.773196   9599.500000  790.090000  39.058000  418.709677   \n",
       "9  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
       "\n",
       "         c_NfL        c_GFAP     c_pT231   c_pT217  \n",
       "0   373.960000   8706.690000  487.340000  0.605900  \n",
       "1   735.907250   8163.845946  501.851892  1.254549  \n",
       "2   735.907250   8163.845946  501.851892  1.254549  \n",
       "3   780.569900   9070.057586  614.922874  2.080534  \n",
       "4   756.707209   8165.744286  525.304762  1.457107  \n",
       "5   735.907250   8163.845946  501.851892  1.254549  \n",
       "6   780.569900   9070.057586  614.922874  0.977400  \n",
       "7  1075.590000  37506.300000  609.430000  0.481200  \n",
       "8   780.569900   9070.057586  614.922874  2.080534  \n",
       "9   735.907250   8163.845946  501.851892  1.254549  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csfbiomk1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a38c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_strem2</th>\n",
       "      <th>c_PDGFRbeta</th>\n",
       "      <th>c_Ab40</th>\n",
       "      <th>c_Ab42</th>\n",
       "      <th>c_pT181</th>\n",
       "      <th>c_ttau</th>\n",
       "      <th>c_NfL</th>\n",
       "      <th>c_GFAP</th>\n",
       "      <th>c_pT231</th>\n",
       "      <th>c_pT217</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2035.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>8046.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>598.000000</td>\n",
       "      <td>373.960000</td>\n",
       "      <td>8706.690000</td>\n",
       "      <td>487.340000</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2397.612245</td>\n",
       "      <td>361.773196</td>\n",
       "      <td>9599.500000</td>\n",
       "      <td>790.090000</td>\n",
       "      <td>39.058000</td>\n",
       "      <td>418.709677</td>\n",
       "      <td>780.569900</td>\n",
       "      <td>9070.057586</td>\n",
       "      <td>614.922874</td>\n",
       "      <td>2.080534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2752.976744</td>\n",
       "      <td>410.348837</td>\n",
       "      <td>11663.651163</td>\n",
       "      <td>964.255814</td>\n",
       "      <td>38.311628</td>\n",
       "      <td>393.948718</td>\n",
       "      <td>756.707209</td>\n",
       "      <td>8165.744286</td>\n",
       "      <td>525.304762</td>\n",
       "      <td>1.457107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      c_strem2  c_PDGFRbeta        c_Ab40      c_Ab42    c_pT181      c_ttau  \\\n",
       "0  2035.000000   295.000000   8046.000000  702.000000  24.400000  598.000000   \n",
       "1  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
       "2  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
       "3  2397.612245   361.773196   9599.500000  790.090000  39.058000  418.709677   \n",
       "4  2752.976744   410.348837  11663.651163  964.255814  38.311628  393.948718   \n",
       "\n",
       "        c_NfL       c_GFAP     c_pT231   c_pT217  \n",
       "0  373.960000  8706.690000  487.340000  0.605900  \n",
       "1  735.907250  8163.845946  501.851892  1.254549  \n",
       "2  735.907250  8163.845946  501.851892  1.254549  \n",
       "3  780.569900  9070.057586  614.922874  2.080534  \n",
       "4  756.707209  8165.744286  525.304762  1.457107  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean for each column, ignoring NaNs\n",
    "mean_values = csfbiomk1.mean()\n",
    "\n",
    "csfbiomk1 = csfbiomk1.fillna(mean_values)\n",
    "\n",
    "csfbiomk1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5572bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling Neuropsychiatric Measurements\n",
    "\n",
    "neuropsych_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\neuropsych_m_p_20231117.csv\"\n",
    "neuropsych = pd.read_csv(neuropsych_file_path)\n",
    "neuropsych = pd.DataFrame(neuropsych)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90bdf10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in neuropsych:\n",
      "                   Column  Number of Nulls\n",
      "0              subject_id                0\n",
      "1                   visit                0\n",
      "2             checkin_bin                0\n",
      "3             exposurebin                0\n",
      "4              age_decade                0\n",
      "..                    ...              ...\n",
      "96          explosivity_z               58\n",
      "97       emo_dyscontrol_z               58\n",
      "98          impulsivity_z               58\n",
      "99   affective_lability_z               58\n",
      "100             nbd_tot_z               58\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "neuropsych_nulls = neuropsych.isnull().sum()\n",
    "\n",
    "neuropsych_nulls_df = pd.DataFrame({\n",
    "    'Column': neuropsych_nulls.index,\n",
    "    'Number of Nulls': neuropsych_nulls.values\n",
    "})\n",
    "\n",
    "neuropsych_nulls_df_transposed = neuropsych_nulls_df.T\n",
    "\n",
    "print(\"Null values in neuropsych:\")\n",
    "print(neuropsych_nulls_df)\n",
    "neuropsych_nulls_df_transposed.to_csv(r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\neuropsych_nulls.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cb4ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>visit</th>\n",
       "      <th>checkin_bin</th>\n",
       "      <th>exposurebin</th>\n",
       "      <th>age_decade</th>\n",
       "      <th>racecat_combined</th>\n",
       "      <th>eduyears</th>\n",
       "      <th>totyr_foot</th>\n",
       "      <th>chiiseas_pf</th>\n",
       "      <th>chiiyrs_pf</th>\n",
       "      <th>...</th>\n",
       "      <th>explosivity</th>\n",
       "      <th>emo_dyscontrol</th>\n",
       "      <th>impulsivity</th>\n",
       "      <th>affective_lability</th>\n",
       "      <th>nbd_tot</th>\n",
       "      <th>explosivity_z</th>\n",
       "      <th>emo_dyscontrol_z</th>\n",
       "      <th>impulsivity_z</th>\n",
       "      <th>affective_lability_z</th>\n",
       "      <th>nbd_tot_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4335.4</td>\n",
       "      <td>2167.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.45455</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>31.3636</td>\n",
       "      <td>-1.01754</td>\n",
       "      <td>-1.18960</td>\n",
       "      <td>-0.28658</td>\n",
       "      <td>-0.88620</td>\n",
       "      <td>-1.05786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10363.1</td>\n",
       "      <td>5708.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47500</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.75758</td>\n",
       "      <td>0.86667</td>\n",
       "      <td>71.2311</td>\n",
       "      <td>0.37271</td>\n",
       "      <td>0.88637</td>\n",
       "      <td>1.77238</td>\n",
       "      <td>2.78227</td>\n",
       "      <td>1.77989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6685.4</td>\n",
       "      <td>4863.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61667</td>\n",
       "      <td>0.91667</td>\n",
       "      <td>0.59848</td>\n",
       "      <td>0.46667</td>\n",
       "      <td>64.9621</td>\n",
       "      <td>1.31809</td>\n",
       "      <td>1.71676</td>\n",
       "      <td>0.69143</td>\n",
       "      <td>0.58118</td>\n",
       "      <td>1.33367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7701.2</td>\n",
       "      <td>6448.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.41667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>32.0833</td>\n",
       "      <td>-0.57266</td>\n",
       "      <td>-0.77441</td>\n",
       "      <td>-1.11016</td>\n",
       "      <td>-0.88620</td>\n",
       "      <td>-1.00663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.31061</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>31.9318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  visit  checkin_bin  exposurebin  age_decade  racecat_combined  \\\n",
       "0        1001      1            2            1           1                 5   \n",
       "1        1002      1            2            1           1                 5   \n",
       "2        1003      1            2            1           1                 5   \n",
       "3        1004      1            1            1           2                 5   \n",
       "4        1005      1            3            0           2                 5   \n",
       "\n",
       "   eduyears  totyr_foot  chiiseas_pf  chiiyrs_pf  ...  explosivity  \\\n",
       "0      16.0         7.0       4335.4      2167.7  ...      0.26667   \n",
       "1      15.0        14.0      10363.1      5708.1  ...      0.47500   \n",
       "2      18.0        12.0       6685.4      4863.9  ...      0.61667   \n",
       "3      16.0        16.0       7701.2      6448.9  ...      0.33333   \n",
       "4      21.0         NaN          NaN         NaN  ...      0.26667   \n",
       "\n",
       "   emo_dyscontrol  impulsivity  affective_lability  nbd_tot  explosivity_z  \\\n",
       "0         0.33333      0.45455             0.20000  31.3636       -1.01754   \n",
       "1         0.75000      0.75758             0.86667  71.2311        0.37271   \n",
       "2         0.91667      0.59848             0.46667  64.9621        1.31809   \n",
       "3         0.41667      0.33333             0.20000  32.0833       -0.57266   \n",
       "4         0.50000      0.31061             0.20000  31.9318            NaN   \n",
       "\n",
       "   emo_dyscontrol_z  impulsivity_z  affective_lability_z  nbd_tot_z  \n",
       "0          -1.18960       -0.28658              -0.88620   -1.05786  \n",
       "1           0.88637        1.77238               2.78227    1.77989  \n",
       "2           1.71676        0.69143               0.58118    1.33367  \n",
       "3          -0.77441       -1.11016              -0.88620   -1.00663  \n",
       "4               NaN            NaN                   NaN        NaN  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuropsych.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47af536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>visit</th>\n",
       "      <th>checkin_bin</th>\n",
       "      <th>exposurebin</th>\n",
       "      <th>age_decade</th>\n",
       "      <th>racecat_combined</th>\n",
       "      <th>eduyears</th>\n",
       "      <th>totyr_foot</th>\n",
       "      <th>chiiseas_pf</th>\n",
       "      <th>chiiyrs_pf</th>\n",
       "      <th>...</th>\n",
       "      <th>explosivity</th>\n",
       "      <th>emo_dyscontrol</th>\n",
       "      <th>impulsivity</th>\n",
       "      <th>affective_lability</th>\n",
       "      <th>nbd_tot</th>\n",
       "      <th>explosivity_z</th>\n",
       "      <th>emo_dyscontrol_z</th>\n",
       "      <th>impulsivity_z</th>\n",
       "      <th>affective_lability_z</th>\n",
       "      <th>nbd_tot_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4335.40</td>\n",
       "      <td>2167.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.45455</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>31.3636</td>\n",
       "      <td>-1.017540e+00</td>\n",
       "      <td>-1.189600e+00</td>\n",
       "      <td>-2.865800e-01</td>\n",
       "      <td>-8.862000e-01</td>\n",
       "      <td>-1.057860e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10363.10</td>\n",
       "      <td>5708.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47500</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.75758</td>\n",
       "      <td>0.86667</td>\n",
       "      <td>71.2311</td>\n",
       "      <td>3.727100e-01</td>\n",
       "      <td>8.863700e-01</td>\n",
       "      <td>1.772380e+00</td>\n",
       "      <td>2.782270e+00</td>\n",
       "      <td>1.779890e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6685.40</td>\n",
       "      <td>4863.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61667</td>\n",
       "      <td>0.91667</td>\n",
       "      <td>0.59848</td>\n",
       "      <td>0.46667</td>\n",
       "      <td>64.9621</td>\n",
       "      <td>1.318090e+00</td>\n",
       "      <td>1.716760e+00</td>\n",
       "      <td>6.914300e-01</td>\n",
       "      <td>5.811800e-01</td>\n",
       "      <td>1.333670e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>7701.20</td>\n",
       "      <td>6448.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.41667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>32.0833</td>\n",
       "      <td>-5.726600e-01</td>\n",
       "      <td>-7.744100e-01</td>\n",
       "      <td>-1.110160e+00</td>\n",
       "      <td>-8.862000e-01</td>\n",
       "      <td>-1.006630e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.852778</td>\n",
       "      <td>10914.89</td>\n",
       "      <td>8229.102222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.31061</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>31.9318</td>\n",
       "      <td>5.056180e-07</td>\n",
       "      <td>5.056180e-07</td>\n",
       "      <td>-1.123596e-07</td>\n",
       "      <td>5.617978e-07</td>\n",
       "      <td>5.617978e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  visit  checkin_bin  exposurebin  age_decade  racecat_combined  \\\n",
       "0        1001      1            2            1           1                 5   \n",
       "1        1002      1            2            1           1                 5   \n",
       "2        1003      1            2            1           1                 5   \n",
       "3        1004      1            1            1           2                 5   \n",
       "4        1005      1            3            0           2                 5   \n",
       "\n",
       "   eduyears  totyr_foot  chiiseas_pf   chiiyrs_pf  ...  explosivity  \\\n",
       "0      16.0    7.000000      4335.40  2167.700000  ...      0.26667   \n",
       "1      15.0   14.000000     10363.10  5708.100000  ...      0.47500   \n",
       "2      18.0   12.000000      6685.40  4863.900000  ...      0.61667   \n",
       "3      16.0   16.000000      7701.20  6448.900000  ...      0.33333   \n",
       "4      21.0   15.852778     10914.89  8229.102222  ...      0.26667   \n",
       "\n",
       "   emo_dyscontrol  impulsivity  affective_lability  nbd_tot  explosivity_z  \\\n",
       "0         0.33333      0.45455             0.20000  31.3636  -1.017540e+00   \n",
       "1         0.75000      0.75758             0.86667  71.2311   3.727100e-01   \n",
       "2         0.91667      0.59848             0.46667  64.9621   1.318090e+00   \n",
       "3         0.41667      0.33333             0.20000  32.0833  -5.726600e-01   \n",
       "4         0.50000      0.31061             0.20000  31.9318   5.056180e-07   \n",
       "\n",
       "   emo_dyscontrol_z  impulsivity_z  affective_lability_z     nbd_tot_z  \n",
       "0     -1.189600e+00  -2.865800e-01         -8.862000e-01 -1.057860e+00  \n",
       "1      8.863700e-01   1.772380e+00          2.782270e+00  1.779890e+00  \n",
       "2      1.716760e+00   6.914300e-01          5.811800e-01  1.333670e+00  \n",
       "3     -7.744100e-01  -1.110160e+00         -8.862000e-01 -1.006630e+00  \n",
       "4      5.056180e-07  -1.123596e-07          5.617978e-07  5.617978e-08  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean for each column, ignoring NaNs\n",
    "mean_values = neuropsych.mean()\n",
    "\n",
    "neuropsych = neuropsych.fillna(mean_values)\n",
    "\n",
    "neuropsych.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51b5dc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id              0\n",
       "visit                   0\n",
       "checkin_bin             0\n",
       "exposurebin             0\n",
       "age_decade              0\n",
       "                       ..\n",
       "explosivity_z           0\n",
       "emo_dyscontrol_z        0\n",
       "impulsivity_z           0\n",
       "affective_lability_z    0\n",
       "nbd_tot_z               0\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuropsych_nulls = neuropsych.isnull().sum()\n",
    "neuropsych_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b971c900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bistot</th>\n",
       "      <th>abis_attention</th>\n",
       "      <th>abis_motor</th>\n",
       "      <th>abis_nonplanning</th>\n",
       "      <th>bhstot</th>\n",
       "      <th>bdhi_total</th>\n",
       "      <th>cnstot</th>\n",
       "      <th>bditot</th>\n",
       "      <th>baitot</th>\n",
       "      <th>pcltot</th>\n",
       "      <th>BGLHA_Childhood_Total</th>\n",
       "      <th>BGLHA_Adolescence_Total</th>\n",
       "      <th>BGLHA_Adulthood_Total</th>\n",
       "      <th>nbd_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>31.3636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30</td>\n",
       "      <td>41.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>71.2311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>33</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>64.9621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>32.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>31.9318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bistot  abis_attention  abis_motor  abis_nonplanning  bhstot  bdhi_total  \\\n",
       "0    72.0            12.0         6.0              12.0     0.0        23.0   \n",
       "1    89.0            16.0        11.0              11.0    19.0        35.0   \n",
       "2    82.0            16.0         8.0               9.0     8.0        35.0   \n",
       "3    48.0             6.0         6.0               5.0     6.0        23.0   \n",
       "4    43.0             5.0         4.0               4.0     1.0        23.0   \n",
       "\n",
       "   cnstot  bditot  baitot  pcltot  BGLHA_Childhood_Total  \\\n",
       "0     7.0      14     9.0    19.0                     13   \n",
       "1    17.0      30    41.0    31.0                     20   \n",
       "2    11.0      33    14.0    18.0                     11   \n",
       "3     7.0       4     9.0    17.0                     12   \n",
       "4     7.0       5     1.0     3.0                     11   \n",
       "\n",
       "   BGLHA_Adolescence_Total  BGLHA_Adulthood_Total  nbd_tot  \n",
       "0                       18                     15  31.3636  \n",
       "1                       17                     18  71.2311  \n",
       "2                       12                     14  64.9621  \n",
       "3                       14                     17  32.0833  \n",
       "4                       11                     13  31.9318  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuropsych = neuropsych[['bistot','abis_attention','abis_motor','abis_nonplanning','bhstot','bdhi_total', 'cnstot', 'bditot', 'baitot', 'pcltot', \n",
    "'BGLHA_Childhood_Total', 'BGLHA_Adolescence_Total', 'BGLHA_Adulthood_Total', 'nbd_tot' ]]\n",
    "neuropsych.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c26d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bistot</th>\n",
       "      <th>abis_attention</th>\n",
       "      <th>abis_motor</th>\n",
       "      <th>abis_nonplanning</th>\n",
       "      <th>bhstot</th>\n",
       "      <th>bdhi_total</th>\n",
       "      <th>cnstot</th>\n",
       "      <th>bditot</th>\n",
       "      <th>baitot</th>\n",
       "      <th>pcltot</th>\n",
       "      <th>BGLHA_Childhood_Total</th>\n",
       "      <th>BGLHA_Adolescence_Total</th>\n",
       "      <th>BGLHA_Adulthood_Total</th>\n",
       "      <th>nbd_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.00000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.00000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>61.931624</td>\n",
       "      <td>10.136752</td>\n",
       "      <td>7.34188</td>\n",
       "      <td>8.538462</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>26.25974</td>\n",
       "      <td>12.038462</td>\n",
       "      <td>10.067797</td>\n",
       "      <td>7.884615</td>\n",
       "      <td>14.888412</td>\n",
       "      <td>12.944915</td>\n",
       "      <td>13.779661</td>\n",
       "      <td>15.521186</td>\n",
       "      <td>42.829579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.042626</td>\n",
       "      <td>3.701422</td>\n",
       "      <td>2.40093</td>\n",
       "      <td>3.098492</td>\n",
       "      <td>4.237623</td>\n",
       "      <td>14.06405</td>\n",
       "      <td>4.293812</td>\n",
       "      <td>9.901841</td>\n",
       "      <td>8.824021</td>\n",
       "      <td>15.671780</td>\n",
       "      <td>3.418525</td>\n",
       "      <td>4.137294</td>\n",
       "      <td>4.625965</td>\n",
       "      <td>13.800428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>27.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.579550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>50.416675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>107.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>65.00000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>87.197000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bistot  abis_attention  abis_motor  abis_nonplanning      bhstot  \\\n",
       "count  236.000000      236.000000   236.00000        236.000000  236.000000   \n",
       "mean    61.931624       10.136752     7.34188          8.538462    3.000000   \n",
       "std     14.042626        3.701422     2.40093          3.098492    4.237623   \n",
       "min     33.000000        5.000000     4.00000          4.000000    0.000000   \n",
       "25%     51.000000        7.000000     5.00000          6.000000    0.000000   \n",
       "50%     61.000000       10.000000     7.00000          8.000000    1.000000   \n",
       "75%     72.000000       13.000000     9.00000         11.000000    4.000000   \n",
       "max    107.000000       20.000000    15.00000         16.000000   19.000000   \n",
       "\n",
       "       bdhi_total      cnstot      bditot      baitot      pcltot  \\\n",
       "count   236.00000  236.000000  236.000000  236.000000  236.000000   \n",
       "mean     26.25974   12.038462   10.067797    7.884615   14.888412   \n",
       "std      14.06405    4.293812    9.901841    8.824021   15.671780   \n",
       "min       3.00000    7.000000    0.000000    0.000000    0.000000   \n",
       "25%      16.00000    8.000000    2.000000    1.000000    3.000000   \n",
       "50%      24.00000   11.000000    8.000000    5.000000   10.000000   \n",
       "75%      35.00000   15.000000   16.000000   12.000000   21.250000   \n",
       "max      65.00000   29.000000   42.000000   42.000000   75.000000   \n",
       "\n",
       "       BGLHA_Childhood_Total  BGLHA_Adolescence_Total  BGLHA_Adulthood_Total  \\\n",
       "count             236.000000               236.000000             236.000000   \n",
       "mean               12.944915                13.779661              15.521186   \n",
       "std                 3.418525                 4.137294               4.625965   \n",
       "min                11.000000                11.000000              11.000000   \n",
       "25%                11.000000                11.000000              12.000000   \n",
       "50%                11.500000                12.000000              14.000000   \n",
       "75%                13.000000                15.000000              18.000000   \n",
       "max                32.000000                34.000000              37.000000   \n",
       "\n",
       "          nbd_tot  \n",
       "count  236.000000  \n",
       "mean    42.829579  \n",
       "std     13.800428  \n",
       "min     27.197000  \n",
       "25%     31.363600  \n",
       "50%     38.579550  \n",
       "75%     50.416675  \n",
       "max     87.197000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuropsych.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e665dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236 entries, 0 to 235\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   bistot                   236 non-null    float64\n",
      " 1   abis_attention           236 non-null    float64\n",
      " 2   abis_motor               236 non-null    float64\n",
      " 3   abis_nonplanning         236 non-null    float64\n",
      " 4   bhstot                   236 non-null    float64\n",
      " 5   bdhi_total               236 non-null    float64\n",
      " 6   cnstot                   236 non-null    float64\n",
      " 7   bditot                   236 non-null    int64  \n",
      " 8   baitot                   236 non-null    float64\n",
      " 9   pcltot                   236 non-null    float64\n",
      " 10  BGLHA_Childhood_Total    236 non-null    int64  \n",
      " 11  BGLHA_Adolescence_Total  236 non-null    int64  \n",
      " 12  BGLHA_Adulthood_Total    236 non-null    int64  \n",
      " 13  nbd_tot                  236 non-null    float64\n",
      "dtypes: float64(10), int64(4)\n",
      "memory usage: 25.9 KB\n"
     ]
    }
   ],
   "source": [
    "neuropsych.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab495d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main_path = r'Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers'\n",
    "folder_name = 'blood_biomk1_to_neuropsych'  \n",
    "save_path = os.path.join(main_path, folder_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7ce67bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bistot  abis_attention  abis_motor  abis_nonplanning  bhstot  bdhi_total  \\\n",
      "0    72.0            12.0         6.0              12.0     0.0        23.0   \n",
      "1    89.0            16.0        11.0              11.0    19.0        35.0   \n",
      "2    82.0            16.0         8.0               9.0     8.0        35.0   \n",
      "3    48.0             6.0         6.0               5.0     6.0        23.0   \n",
      "4    43.0             5.0         4.0               4.0     1.0        23.0   \n",
      "\n",
      "   cnstot  bditot  baitot  pcltot  BGLHA_Childhood_Total  \\\n",
      "0     7.0      14     9.0    19.0                     13   \n",
      "1    17.0      30    41.0    31.0                     20   \n",
      "2    11.0      33    14.0    18.0                     11   \n",
      "3     7.0       4     9.0    17.0                     12   \n",
      "4     7.0       5     1.0     3.0                     11   \n",
      "\n",
      "   BGLHA_Adolescence_Total  BGLHA_Adulthood_Total  nbd_tot  category  \n",
      "0                       18                     15  31.3636         2  \n",
      "1                       17                     18  71.2311         2  \n",
      "2                       12                     14  64.9621         2  \n",
      "3                       14                     17  32.0833         1  \n",
      "4                       11                     13  31.9318         3  \n",
      "      c_strem2  c_PDGFRbeta        c_Ab40      c_Ab42    c_pT181      c_ttau  \\\n",
      "0  2035.000000   295.000000   8046.000000  702.000000  24.400000  598.000000   \n",
      "1  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
      "2  2683.750000   344.650000  10146.975000  863.500000  32.220000  401.702703   \n",
      "3  2397.612245   361.773196   9599.500000  790.090000  39.058000  418.709677   \n",
      "4  2752.976744   410.348837  11663.651163  964.255814  38.311628  393.948718   \n",
      "\n",
      "        c_NfL       c_GFAP     c_pT231   c_pT217  category  \n",
      "0  373.960000  8706.690000  487.340000  0.605900         2  \n",
      "1  735.907250  8163.845946  501.851892  1.254549         2  \n",
      "2  735.907250  8163.845946  501.851892  1.254549         2  \n",
      "3  780.569900  9070.057586  614.922874  2.080534         1  \n",
      "4  756.707209  8165.744286  525.304762  1.457107         3  \n"
     ]
    }
   ],
   "source": [
    "categories_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\categories.csv\"\n",
    "categories_df = pd.read_csv(categories_file_path)\n",
    "new_column = categories_df['checkin_bin']\n",
    "neuropsych['category'] = new_column\n",
    "print(neuropsych.head())\n",
    "\n",
    "csfbiomk1['category'] = new_column\n",
    "print(csfbiomk1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "103b2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csfbiomk1['category'] = neuropsych['category']\n",
    "\n",
    "# correlation_matrix = csfbiomk1.corr()\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "# plt.title('Correlation Heatmap of Features')\n",
    "# plt.show()\n",
    "\n",
    "# # Alternatively, scatter plots for each feature vs y (checkin_bin)\n",
    "# for feature in csfbiomk1.columns[:-1]:  # Exclude the label column\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(x='category', y=feature, data=csfbiomk1)\n",
    "#     plt.title(f'Boxplot of {feature} by category')\n",
    "#     plt.xlabel('category')\n",
    "#     plt.ylabel(feature)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44bd65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c6f4a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_strem2</th>\n",
       "      <th>c_PDGFRbeta</th>\n",
       "      <th>c_Ab40</th>\n",
       "      <th>c_Ab42</th>\n",
       "      <th>c_pT181</th>\n",
       "      <th>c_ttau</th>\n",
       "      <th>c_NfL</th>\n",
       "      <th>c_GFAP</th>\n",
       "      <th>c_pT231</th>\n",
       "      <th>c_pT217</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2035.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>8046.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>598.000000</td>\n",
       "      <td>373.960000</td>\n",
       "      <td>8706.690000</td>\n",
       "      <td>487.340000</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2397.612245</td>\n",
       "      <td>361.773196</td>\n",
       "      <td>9599.500000</td>\n",
       "      <td>790.090000</td>\n",
       "      <td>39.058000</td>\n",
       "      <td>418.709677</td>\n",
       "      <td>780.569900</td>\n",
       "      <td>9070.057586</td>\n",
       "      <td>614.922874</td>\n",
       "      <td>2.080534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2752.976744</td>\n",
       "      <td>410.348837</td>\n",
       "      <td>11663.651163</td>\n",
       "      <td>964.255814</td>\n",
       "      <td>38.311628</td>\n",
       "      <td>393.948718</td>\n",
       "      <td>756.707209</td>\n",
       "      <td>8165.744286</td>\n",
       "      <td>525.304762</td>\n",
       "      <td>1.457107</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>607.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>6679.000000</td>\n",
       "      <td>621.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>273.150000</td>\n",
       "      <td>954.370000</td>\n",
       "      <td>343.900000</td>\n",
       "      <td>0.572700</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>2231.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>9686.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>423.170000</td>\n",
       "      <td>3147.720000</td>\n",
       "      <td>340.060000</td>\n",
       "      <td>1.457107</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>987.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>5283.000000</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>12.900000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>220.620000</td>\n",
       "      <td>1691.780000</td>\n",
       "      <td>183.410000</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3525.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>9057.000000</td>\n",
       "      <td>810.000000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>954.590000</td>\n",
       "      <td>5463.900000</td>\n",
       "      <td>499.050000</td>\n",
       "      <td>2.080534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3091.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>11078.000000</td>\n",
       "      <td>818.000000</td>\n",
       "      <td>43.700000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>580.630000</td>\n",
       "      <td>3599.320000</td>\n",
       "      <td>816.360000</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        c_strem2  c_PDGFRbeta        c_Ab40      c_Ab42    c_pT181  \\\n",
       "0    2035.000000   295.000000   8046.000000  702.000000  24.400000   \n",
       "1    2683.750000   344.650000  10146.975000  863.500000  32.220000   \n",
       "2    2683.750000   344.650000  10146.975000  863.500000  32.220000   \n",
       "3    2397.612245   361.773196   9599.500000  790.090000  39.058000   \n",
       "4    2752.976744   410.348837  11663.651163  964.255814  38.311628   \n",
       "..           ...          ...           ...         ...        ...   \n",
       "231   607.000000   279.000000   6679.000000  621.000000  18.000000   \n",
       "232  2231.000000   351.000000   9686.000000  801.000000  26.200000   \n",
       "233   987.000000   201.000000   5283.000000  536.000000  12.900000   \n",
       "234  3525.000000   477.000000   9057.000000  810.000000  28.400000   \n",
       "235  3091.000000   408.000000  11078.000000  818.000000  43.700000   \n",
       "\n",
       "         c_ttau       c_NfL       c_GFAP     c_pT231   c_pT217  category  \n",
       "0    598.000000  373.960000  8706.690000  487.340000  0.605900         2  \n",
       "1    401.702703  735.907250  8163.845946  501.851892  1.254549         2  \n",
       "2    401.702703  735.907250  8163.845946  501.851892  1.254549         2  \n",
       "3    418.709677  780.569900  9070.057586  614.922874  2.080534         1  \n",
       "4    393.948718  756.707209  8165.744286  525.304762  1.457107         3  \n",
       "..          ...         ...          ...         ...       ...       ...  \n",
       "231  401.702703  273.150000   954.370000  343.900000  0.572700         2  \n",
       "232  348.000000  423.170000  3147.720000  340.060000  1.457107         3  \n",
       "233  401.702703  220.620000  1691.780000  183.410000  0.111200         2  \n",
       "234  186.000000  954.590000  5463.900000  499.050000  2.080534         1  \n",
       "235  329.000000  580.630000  3599.320000  816.360000  0.952700         3  \n",
       "\n",
       "[236 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csfbiomk1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca52a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0180ae7c",
   "metadata": {},
   "source": [
    "\n",
    "# 95th percentile for both analysis and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a17bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in csfbiomk1.columns[:-1]:  # Assuming 'category' is the last column\n",
    "    upper_limit = csfbiomk1[feature].quantile(0.95)  # Cap at 95th percentile\n",
    "    lower_limit = csfbiomk1[feature].quantile(0.05)  # Floor at 5th percentile\n",
    "    csfbiomk1[feature] = np.where(csfbiomk1[feature] > upper_limit, upper_limit, csfbiomk1[feature])\n",
    "    csfbiomk1[feature] = np.where(csfbiomk1[feature] < lower_limit, lower_limit, csfbiomk1[feature])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38819f",
   "metadata": {},
   "source": [
    "# Initial Step for Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a2d78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = csfbiomk1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09f870dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_strem2</th>\n",
       "      <th>c_PDGFRbeta</th>\n",
       "      <th>c_Ab40</th>\n",
       "      <th>c_Ab42</th>\n",
       "      <th>c_pT181</th>\n",
       "      <th>c_ttau</th>\n",
       "      <th>c_NfL</th>\n",
       "      <th>c_GFAP</th>\n",
       "      <th>c_pT231</th>\n",
       "      <th>c_pT217</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2035.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>8046.000000</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>598.000000</td>\n",
       "      <td>373.960000</td>\n",
       "      <td>8706.690000</td>\n",
       "      <td>487.340000</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2683.750000</td>\n",
       "      <td>344.650000</td>\n",
       "      <td>10146.975000</td>\n",
       "      <td>863.500000</td>\n",
       "      <td>32.220000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>735.907250</td>\n",
       "      <td>8163.845946</td>\n",
       "      <td>501.851892</td>\n",
       "      <td>1.254549</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2397.612245</td>\n",
       "      <td>361.773196</td>\n",
       "      <td>9599.500000</td>\n",
       "      <td>790.090000</td>\n",
       "      <td>39.058000</td>\n",
       "      <td>418.709677</td>\n",
       "      <td>780.569900</td>\n",
       "      <td>9070.057586</td>\n",
       "      <td>614.922874</td>\n",
       "      <td>2.080534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2752.976744</td>\n",
       "      <td>410.348837</td>\n",
       "      <td>11663.651163</td>\n",
       "      <td>964.255814</td>\n",
       "      <td>38.311628</td>\n",
       "      <td>393.948718</td>\n",
       "      <td>756.707209</td>\n",
       "      <td>8165.744286</td>\n",
       "      <td>525.304762</td>\n",
       "      <td>1.457107</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1109.750000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>6679.000000</td>\n",
       "      <td>621.000000</td>\n",
       "      <td>19.575000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>273.150000</td>\n",
       "      <td>1900.257500</td>\n",
       "      <td>343.900000</td>\n",
       "      <td>0.572700</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>2231.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>9686.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>26.200000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>423.170000</td>\n",
       "      <td>3147.720000</td>\n",
       "      <td>340.060000</td>\n",
       "      <td>1.457107</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>1109.750000</td>\n",
       "      <td>208.750000</td>\n",
       "      <td>5874.500000</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>19.575000</td>\n",
       "      <td>401.702703</td>\n",
       "      <td>272.692500</td>\n",
       "      <td>1900.257500</td>\n",
       "      <td>278.202500</td>\n",
       "      <td>0.318225</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3525.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>9057.000000</td>\n",
       "      <td>810.000000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>189.500000</td>\n",
       "      <td>954.590000</td>\n",
       "      <td>5463.900000</td>\n",
       "      <td>499.050000</td>\n",
       "      <td>2.080534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3091.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>11078.000000</td>\n",
       "      <td>818.000000</td>\n",
       "      <td>43.700000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>580.630000</td>\n",
       "      <td>3599.320000</td>\n",
       "      <td>816.360000</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        c_strem2  c_PDGFRbeta        c_Ab40      c_Ab42    c_pT181  \\\n",
       "0    2035.000000   295.000000   8046.000000  702.000000  24.400000   \n",
       "1    2683.750000   344.650000  10146.975000  863.500000  32.220000   \n",
       "2    2683.750000   344.650000  10146.975000  863.500000  32.220000   \n",
       "3    2397.612245   361.773196   9599.500000  790.090000  39.058000   \n",
       "4    2752.976744   410.348837  11663.651163  964.255814  38.311628   \n",
       "..           ...          ...           ...         ...        ...   \n",
       "231  1109.750000   279.000000   6679.000000  621.000000  19.575000   \n",
       "232  2231.000000   351.000000   9686.000000  801.000000  26.200000   \n",
       "233  1109.750000   208.750000   5874.500000  536.000000  19.575000   \n",
       "234  3525.000000   477.000000   9057.000000  810.000000  28.400000   \n",
       "235  3091.000000   408.000000  11078.000000  818.000000  43.700000   \n",
       "\n",
       "         c_ttau       c_NfL       c_GFAP     c_pT231   c_pT217  category  \n",
       "0    598.000000  373.960000  8706.690000  487.340000  0.605900         2  \n",
       "1    401.702703  735.907250  8163.845946  501.851892  1.254549         2  \n",
       "2    401.702703  735.907250  8163.845946  501.851892  1.254549         2  \n",
       "3    418.709677  780.569900  9070.057586  614.922874  2.080534         1  \n",
       "4    393.948718  756.707209  8165.744286  525.304762  1.457107         3  \n",
       "..          ...         ...          ...         ...       ...       ...  \n",
       "231  401.702703  273.150000  1900.257500  343.900000  0.572700         2  \n",
       "232  348.000000  423.170000  3147.720000  340.060000  1.457107         3  \n",
       "233  401.702703  272.692500  1900.257500  278.202500  0.318225         2  \n",
       "234  189.500000  954.590000  5463.900000  499.050000  2.080534         1  \n",
       "235  329.000000  580.630000  3599.320000  816.360000  0.952700         3  \n",
       "\n",
       "[236 rows x 11 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08314cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# got error due to non numeric values, so removing them:\n",
    "\n",
    "# Assume csfbiomk1 is your DataFrame loaded with various types of data\n",
    "X = csfbiomk1.copy()\n",
    "\n",
    "# Select only numeric columns from the DataFrame\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "X = X_numeric\n",
    "\n",
    "# Now X_numeric contains only the numeric columns of the original DataFrame\n",
    "# Proceed with scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd00cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns you want to select for modeling\n",
    "## columns = ['bistot','abis_attention','abis_motor','abis_nonplanning','bhstot','bdhi_total', 'cnstot', 'bditot', 'baitot', 'pcltot', 'BGLHA_Childhood_Total', 'BGLHA_Adolescence_Total', 'BGLHA_Adulthood_Total', 'nbd_tot' ]\n",
    "\n",
    "selected_columns = ['nbd_tot' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07f9aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nbd_tot']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70beb6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to feature_importance_rankings.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Random Forest\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# First, calculate feature importances for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train full model to get feature importances\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importance and rank features\n",
    "    feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Save performance metrics with all features as a baseline\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to feature_importance_rankings.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ef49486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Multiple Linear Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for linear regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb96033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 85\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 126\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 164\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 203\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 246\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 288\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 407\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Gradient Boosting Machines\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = LGBMRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b43ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"XGBoost\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = xgb.XGBRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = xgb.XGBRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d010d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Support Vector Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for SVR\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train SVR model with all features as a baseline\n",
    "    model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28cba56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000238024A5BD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 997us/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000238024A4550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 998us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 998us/step\n",
      "3/3 [==============================] - 0s 996us/step\n",
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"ANN\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for ANN\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train ANN model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=X_train_selected.shape[1]),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train_selected, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test_selected).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Scale selected features for ANN\n",
    "        X_train_selected_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_selected_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        # Train and evaluate ANN model with selected features\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_dim=X_train_selected_scaled.shape[1]),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train_selected_scaled, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "        \n",
    "        y_pred = model.predict(X_test_selected_scaled).flatten()\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bd72f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"KNeighbors Regressor\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for KNN\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train KNN model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate KNN model with selected features\n",
    "        model = KNeighborsRegressor(n_neighbors=5)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "294b6b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Stacking Regressor\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base models and final estimator for stacking\n",
    "base_models = [\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=13, leaf_size=23, p=1, weights='uniform')),\n",
    "    ('svr', SVR(kernel='rbf', C=0.2, epsilon=0.01))\n",
    "]\n",
    "final_estimator = Ridge()\n",
    "stack_model = StackingRegressor(estimators=base_models, final_estimator=final_estimator, cv=5)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train stacking model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    stack_model.fit(X_train_selected, y_train)\n",
    "    y_pred = stack_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate stacking model with selected features\n",
    "        stack_model.fit(X_train_selected, y_train)\n",
    "        y_pred = stack_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aec7c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bagging with SVR\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base model for Bagging\n",
    "base_model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train Bagging model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "    bagging_model.fit(X_train_selected, y_train)\n",
    "    y_pred = bagging_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate Bagging model with selected features\n",
    "        bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "        bagging_model.fit(X_train_selected, y_train)\n",
    "        y_pred = bagging_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "628de5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bagging with SVR\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base model for Bagging\n",
    "base_model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train Bagging model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "    bagging_model.fit(X_train_selected, y_train)\n",
    "    y_pred = bagging_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate Bagging model with selected features\n",
    "        bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "        bagging_model.fit(X_train_selected, y_train)\n",
    "        y_pred = bagging_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f0979a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Lasso Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for Lasso regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Lasso Regression Model with L1 regularization for feature selection\n",
    "    model = Lasso(alpha=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Select features based on non-zero coefficients\n",
    "    selector = SelectFromModel(model, prefit=True)\n",
    "    selected_features = X_train.columns[selector.get_support()]\n",
    "    \n",
    "    # Feature importance ranking based on absolute Lasso coefficients\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "\n",
    "    # Evaluate baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = Lasso(alpha=0.1)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aeecddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initialize lists and dictionaries to store results and feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Ridge Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for Ridge regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Ridge Regression Model\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use coefficients for feature importance\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Evaluate baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = Ridge(alpha=1.0)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "112d9cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize lists and dictionaries for storing results and feature rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bayesian Ridge\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for consistency\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train full model to get feature importances\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use coefficients for feature importance ranking\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = BayesianRidge()\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0464b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Gaussian Process Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the kernel\n",
    "kernel = RBF(1.0)\n",
    "\n",
    "# Standardize features for Gaussian Process Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train the full Gaussian Process model as a baseline\n",
    "    model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_csfbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_csfbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b09f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
