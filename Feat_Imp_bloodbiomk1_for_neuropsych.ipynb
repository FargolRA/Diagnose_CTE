{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af7ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c94958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling biomarkers\n",
    "bloodbiomk1_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\bloodbiomk1_20231117.csv\"\n",
    "bloodbiomk1 = pd.read_csv(bloodbiomk1_file_path)\n",
    "bloodbiomk1 = pd.DataFrame(bloodbiomk1)\n",
    "\n",
    "bloodbiomk2_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\bloodbiomk2_20231117.csv\"\n",
    "bloodbiomk2 = pd.read_csv(bloodbiomk2_file_path)\n",
    "bloodbiomk2 = pd.DataFrame(bloodbiomk2)\n",
    "\n",
    "csfbiomk1_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk1_20231117.csv\"\n",
    "csfbiomk1 = pd.read_csv(csfbiomk1_file_path)\n",
    "csfbiomk1 = pd.DataFrame(csfbiomk1)\n",
    "\n",
    "csfbiomk2_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk2_20231117.csv\"\n",
    "csfbiomk2 = pd.read_csv(csfbiomk2_file_path)\n",
    "csfbiomk2 = pd.DataFrame(csfbiomk2)\n",
    "\n",
    "csfbiomk3_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk3_20231117.csv\"\n",
    "csfbiomk3 = pd.read_csv(csfbiomk3_file_path)\n",
    "csfbiomk3 = pd.DataFrame(csfbiomk3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae0e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main_path = r'Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers'\n",
    "folder_name = 'blood_biomk1_to_neuropsych'  \n",
    "save_path = os.path.join(main_path, folder_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# save_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\blood_biomk1_neuropsych\" # Update this path to your desired folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c104242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in bloodbiomk1:\n",
      "                           Column  Number of Nulls\n",
      "0                      subject_id                0\n",
      "1                           visit                0\n",
      "2                     checkin_bin                0\n",
      "3                     exposurebin                0\n",
      "4                      age_decade                0\n",
      "5                racecat_combined                0\n",
      "6                        eduyears                0\n",
      "7                      totyr_foot               56\n",
      "8                     chiiseas_pf               56\n",
      "9                      chiiyrs_pf               56\n",
      "10                    chiiseas_pl               56\n",
      "11                     chiiyrs_pl               56\n",
      "12                    chiiseas_pg               56\n",
      "13                     chiiyrs_pg               56\n",
      "14                         p_Ab40                4\n",
      "15                         p_Ab42                5\n",
      "16                         p_GFAP                8\n",
      "17                          p_NfL                4\n",
      "18    p_Ab40_FLAG_below_ref_range                4\n",
      "19    p_Ab42_FLAG_below_ref_range                4\n",
      "20    p_GFAP_FLAG_below_ref_range                4\n",
      "21     p_NfL_FLAG_below_ref_range                4\n",
      "22                    p_PDGFRbeta                5\n",
      "23         p_PDGFRbeta_FLAG_other              225\n",
      "24                        p_pT181                4\n",
      "25                        p_pT231                4\n",
      "26                         p_ttau               10\n",
      "27  dxcte_totaltau_plasma_flag_dv               10\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "bloodbiomk1_nulls = bloodbiomk1.isnull().sum()\n",
    "\n",
    "bloodbiomk1_nulls_df = pd.DataFrame({\n",
    "    'Column': bloodbiomk1_nulls.index,\n",
    "    'Number of Nulls': bloodbiomk1_nulls.values\n",
    "})\n",
    "\n",
    "bloodbiomk1_nulls_df_transposed = bloodbiomk1_nulls_df.T\n",
    "\n",
    "print(\"Null values in bloodbiomk1:\")\n",
    "print(bloodbiomk1_nulls_df)\n",
    "bloodbiomk1_nulls_df_transposed.to_csv(r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\bloodbiomk1_nulls.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f107621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_Ab40_FLAG_below_ref_range</th>\n",
       "      <th>p_Ab42_FLAG_below_ref_range</th>\n",
       "      <th>p_GFAP_FLAG_below_ref_range</th>\n",
       "      <th>p_NfL_FLAG_below_ref_range</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "      <th>dxcte_totaltau_plasma_flag_dv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.5</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.12</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.2</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.04</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.5</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.0</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Ab40  p_Ab42  p_GFAP  p_NfL  p_Ab40_FLAG_below_ref_range  \\\n",
       "0   126.5   8.535   73.80  11.95                          0.0   \n",
       "1    84.2   7.460   42.05   6.44                          0.0   \n",
       "2   124.0   7.285   34.35   8.22                          0.0   \n",
       "3   110.5   5.920   48.45  13.00                          0.0   \n",
       "4   126.0   9.150   46.75  11.20                          0.0   \n",
       "\n",
       "   p_Ab42_FLAG_below_ref_range  p_GFAP_FLAG_below_ref_range  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   p_NfL_FLAG_below_ref_range  p_PDGFRbeta  p_pT181  p_pT231  p_ttau  \\\n",
       "0                         0.0      5628.71     8.12     7.58    1.25   \n",
       "1                         0.0     10123.09     6.04     7.53    1.20   \n",
       "2                         0.0     10045.39     9.33     9.34    1.57   \n",
       "3                         0.0      9563.19     6.63     5.87    0.73   \n",
       "4                         0.0     12826.15     8.31     7.99    1.54   \n",
       "\n",
       "   dxcte_totaltau_plasma_flag_dv  \n",
       "0                            1.0  \n",
       "1                            1.0  \n",
       "2                            1.0  \n",
       "3                            1.0  \n",
       "4                            1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloodbiomk1 = bloodbiomk1.drop(columns=bloodbiomk1.loc[:,'subject_id':'chiiyrs_pg'].columns)\n",
    "bloodbiomk1 = bloodbiomk1.drop(columns='p_PDGFRbeta_FLAG_other')\n",
    "\n",
    "bloodbiomk1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045b72a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236 entries, 0 to 235\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   p_Ab40       232 non-null    float64\n",
      " 1   p_Ab42       231 non-null    float64\n",
      " 2   p_GFAP       228 non-null    float64\n",
      " 3   p_NfL        232 non-null    float64\n",
      " 4   p_PDGFRbeta  231 non-null    float64\n",
      " 5   p_pT181      232 non-null    float64\n",
      " 6   p_pT231      232 non-null    float64\n",
      " 7   p_ttau       226 non-null    float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 14.9 KB\n"
     ]
    }
   ],
   "source": [
    "bloodbiomk1 = bloodbiomk1.drop(columns=['p_Ab40_FLAG_below_ref_range',\n",
    "                              'p_Ab42_FLAG_below_ref_range',\n",
    "                              'p_GFAP_FLAG_below_ref_range', \n",
    "                              'p_NfL_FLAG_below_ref_range',\n",
    "                              'dxcte_totaltau_plasma_flag_dv'])\n",
    "bloodbiomk1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d355d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.50</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.950</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.12</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.20</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.440</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.04</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.00</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.220</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.50</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.000</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.00</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.200</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.60</td>\n",
       "      <td>3.565</td>\n",
       "      <td>20.80</td>\n",
       "      <td>50.300</td>\n",
       "      <td>4625.97</td>\n",
       "      <td>6.36</td>\n",
       "      <td>5.98</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69.50</td>\n",
       "      <td>5.985</td>\n",
       "      <td>66.50</td>\n",
       "      <td>4.765</td>\n",
       "      <td>9019.82</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5.69</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>76.85</td>\n",
       "      <td>5.465</td>\n",
       "      <td>98.10</td>\n",
       "      <td>9.330</td>\n",
       "      <td>4259.99</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>131.50</td>\n",
       "      <td>9.860</td>\n",
       "      <td>73.70</td>\n",
       "      <td>15.800</td>\n",
       "      <td>7806.22</td>\n",
       "      <td>6.47</td>\n",
       "      <td>3.74</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85.00</td>\n",
       "      <td>7.900</td>\n",
       "      <td>21.70</td>\n",
       "      <td>7.975</td>\n",
       "      <td>6606.37</td>\n",
       "      <td>8.53</td>\n",
       "      <td>6.15</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Ab40  p_Ab42  p_GFAP   p_NfL  p_PDGFRbeta  p_pT181  p_pT231  p_ttau\n",
       "0  126.50   8.535   73.80  11.950      5628.71     8.12     7.58    1.25\n",
       "1   84.20   7.460   42.05   6.440     10123.09     6.04     7.53    1.20\n",
       "2  124.00   7.285   34.35   8.220     10045.39     9.33     9.34    1.57\n",
       "3  110.50   5.920   48.45  13.000      9563.19     6.63     5.87    0.73\n",
       "4  126.00   9.150   46.75  11.200     12826.15     8.31     7.99    1.54\n",
       "5   42.60   3.565   20.80  50.300      4625.97     6.36     5.98    1.74\n",
       "6   69.50   5.985   66.50   4.765      9019.82     5.30     5.69    1.33\n",
       "7   76.85   5.465   98.10   9.330      4259.99     5.87     3.78    1.10\n",
       "8  131.50   9.860   73.70  15.800      7806.22     6.47     3.74    2.14\n",
       "9   85.00   7.900   21.70   7.975      6606.37     8.53     6.15    1.38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloodbiomk1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a38c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.5</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.95</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.12</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.2</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.44</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.04</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.22</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.5</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.00</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.0</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.20</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Ab40  p_Ab42  p_GFAP  p_NfL  p_PDGFRbeta  p_pT181  p_pT231  p_ttau\n",
       "0   126.5   8.535   73.80  11.95      5628.71     8.12     7.58    1.25\n",
       "1    84.2   7.460   42.05   6.44     10123.09     6.04     7.53    1.20\n",
       "2   124.0   7.285   34.35   8.22     10045.39     9.33     9.34    1.57\n",
       "3   110.5   5.920   48.45  13.00      9563.19     6.63     5.87    0.73\n",
       "4   126.0   9.150   46.75  11.20     12826.15     8.31     7.99    1.54"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean for each column, ignoring NaNs\n",
    "mean_values = bloodbiomk1.mean()\n",
    "\n",
    "bloodbiomk1 = bloodbiomk1.fillna(mean_values)\n",
    "\n",
    "bloodbiomk1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5572bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling Neuropsychiatric Measurements\n",
    "\n",
    "neuropsych_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\neuropsych_m_p_20231117.csv\"\n",
    "neuropsych = pd.read_csv(neuropsych_file_path)\n",
    "neuropsych = pd.DataFrame(neuropsych)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90bdf10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in neuropsych:\n",
      "                   Column  Number of Nulls\n",
      "0              subject_id                0\n",
      "1                   visit                0\n",
      "2             checkin_bin                0\n",
      "3             exposurebin                0\n",
      "4              age_decade                0\n",
      "..                    ...              ...\n",
      "96          explosivity_z               58\n",
      "97       emo_dyscontrol_z               58\n",
      "98          impulsivity_z               58\n",
      "99   affective_lability_z               58\n",
      "100             nbd_tot_z               58\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "neuropsych_nulls = neuropsych.isnull().sum()\n",
    "\n",
    "neuropsych_nulls_df = pd.DataFrame({\n",
    "    'Column': neuropsych_nulls.index,\n",
    "    'Number of Nulls': neuropsych_nulls.values\n",
    "})\n",
    "\n",
    "neuropsych_nulls_df_transposed = neuropsych_nulls_df.T\n",
    "\n",
    "print(\"Null values in neuropsych:\")\n",
    "print(neuropsych_nulls_df)\n",
    "neuropsych_nulls_df_transposed.to_csv(r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\neuropsych_nulls.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cb4ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>visit</th>\n",
       "      <th>checkin_bin</th>\n",
       "      <th>exposurebin</th>\n",
       "      <th>age_decade</th>\n",
       "      <th>racecat_combined</th>\n",
       "      <th>eduyears</th>\n",
       "      <th>totyr_foot</th>\n",
       "      <th>chiiseas_pf</th>\n",
       "      <th>chiiyrs_pf</th>\n",
       "      <th>...</th>\n",
       "      <th>explosivity</th>\n",
       "      <th>emo_dyscontrol</th>\n",
       "      <th>impulsivity</th>\n",
       "      <th>affective_lability</th>\n",
       "      <th>nbd_tot</th>\n",
       "      <th>explosivity_z</th>\n",
       "      <th>emo_dyscontrol_z</th>\n",
       "      <th>impulsivity_z</th>\n",
       "      <th>affective_lability_z</th>\n",
       "      <th>nbd_tot_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4335.4</td>\n",
       "      <td>2167.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.45455</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>31.3636</td>\n",
       "      <td>-1.01754</td>\n",
       "      <td>-1.18960</td>\n",
       "      <td>-0.28658</td>\n",
       "      <td>-0.88620</td>\n",
       "      <td>-1.05786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10363.1</td>\n",
       "      <td>5708.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47500</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.75758</td>\n",
       "      <td>0.86667</td>\n",
       "      <td>71.2311</td>\n",
       "      <td>0.37271</td>\n",
       "      <td>0.88637</td>\n",
       "      <td>1.77238</td>\n",
       "      <td>2.78227</td>\n",
       "      <td>1.77989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6685.4</td>\n",
       "      <td>4863.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61667</td>\n",
       "      <td>0.91667</td>\n",
       "      <td>0.59848</td>\n",
       "      <td>0.46667</td>\n",
       "      <td>64.9621</td>\n",
       "      <td>1.31809</td>\n",
       "      <td>1.71676</td>\n",
       "      <td>0.69143</td>\n",
       "      <td>0.58118</td>\n",
       "      <td>1.33367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7701.2</td>\n",
       "      <td>6448.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.41667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>32.0833</td>\n",
       "      <td>-0.57266</td>\n",
       "      <td>-0.77441</td>\n",
       "      <td>-1.11016</td>\n",
       "      <td>-0.88620</td>\n",
       "      <td>-1.00663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.31061</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>31.9318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  visit  checkin_bin  exposurebin  age_decade  racecat_combined  \\\n",
       "0        1001      1            2            1           1                 5   \n",
       "1        1002      1            2            1           1                 5   \n",
       "2        1003      1            2            1           1                 5   \n",
       "3        1004      1            1            1           2                 5   \n",
       "4        1005      1            3            0           2                 5   \n",
       "\n",
       "   eduyears  totyr_foot  chiiseas_pf  chiiyrs_pf  ...  explosivity  \\\n",
       "0      16.0         7.0       4335.4      2167.7  ...      0.26667   \n",
       "1      15.0        14.0      10363.1      5708.1  ...      0.47500   \n",
       "2      18.0        12.0       6685.4      4863.9  ...      0.61667   \n",
       "3      16.0        16.0       7701.2      6448.9  ...      0.33333   \n",
       "4      21.0         NaN          NaN         NaN  ...      0.26667   \n",
       "\n",
       "   emo_dyscontrol  impulsivity  affective_lability  nbd_tot  explosivity_z  \\\n",
       "0         0.33333      0.45455             0.20000  31.3636       -1.01754   \n",
       "1         0.75000      0.75758             0.86667  71.2311        0.37271   \n",
       "2         0.91667      0.59848             0.46667  64.9621        1.31809   \n",
       "3         0.41667      0.33333             0.20000  32.0833       -0.57266   \n",
       "4         0.50000      0.31061             0.20000  31.9318            NaN   \n",
       "\n",
       "   emo_dyscontrol_z  impulsivity_z  affective_lability_z  nbd_tot_z  \n",
       "0          -1.18960       -0.28658              -0.88620   -1.05786  \n",
       "1           0.88637        1.77238               2.78227    1.77989  \n",
       "2           1.71676        0.69143               0.58118    1.33367  \n",
       "3          -0.77441       -1.11016              -0.88620   -1.00663  \n",
       "4               NaN            NaN                   NaN        NaN  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuropsych.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47af536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>visit</th>\n",
       "      <th>checkin_bin</th>\n",
       "      <th>exposurebin</th>\n",
       "      <th>age_decade</th>\n",
       "      <th>racecat_combined</th>\n",
       "      <th>eduyears</th>\n",
       "      <th>totyr_foot</th>\n",
       "      <th>chiiseas_pf</th>\n",
       "      <th>chiiyrs_pf</th>\n",
       "      <th>...</th>\n",
       "      <th>explosivity</th>\n",
       "      <th>emo_dyscontrol</th>\n",
       "      <th>impulsivity</th>\n",
       "      <th>affective_lability</th>\n",
       "      <th>nbd_tot</th>\n",
       "      <th>explosivity_z</th>\n",
       "      <th>emo_dyscontrol_z</th>\n",
       "      <th>impulsivity_z</th>\n",
       "      <th>affective_lability_z</th>\n",
       "      <th>nbd_tot_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4335.40</td>\n",
       "      <td>2167.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.45455</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>31.3636</td>\n",
       "      <td>-1.017540e+00</td>\n",
       "      <td>-1.189600e+00</td>\n",
       "      <td>-2.865800e-01</td>\n",
       "      <td>-8.862000e-01</td>\n",
       "      <td>-1.057860e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10363.10</td>\n",
       "      <td>5708.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47500</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.75758</td>\n",
       "      <td>0.86667</td>\n",
       "      <td>71.2311</td>\n",
       "      <td>3.727100e-01</td>\n",
       "      <td>8.863700e-01</td>\n",
       "      <td>1.772380e+00</td>\n",
       "      <td>2.782270e+00</td>\n",
       "      <td>1.779890e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6685.40</td>\n",
       "      <td>4863.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61667</td>\n",
       "      <td>0.91667</td>\n",
       "      <td>0.59848</td>\n",
       "      <td>0.46667</td>\n",
       "      <td>64.9621</td>\n",
       "      <td>1.318090e+00</td>\n",
       "      <td>1.716760e+00</td>\n",
       "      <td>6.914300e-01</td>\n",
       "      <td>5.811800e-01</td>\n",
       "      <td>1.333670e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>7701.20</td>\n",
       "      <td>6448.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.41667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>32.0833</td>\n",
       "      <td>-5.726600e-01</td>\n",
       "      <td>-7.744100e-01</td>\n",
       "      <td>-1.110160e+00</td>\n",
       "      <td>-8.862000e-01</td>\n",
       "      <td>-1.006630e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.852778</td>\n",
       "      <td>10914.89</td>\n",
       "      <td>8229.102222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26667</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.31061</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>31.9318</td>\n",
       "      <td>5.056180e-07</td>\n",
       "      <td>5.056180e-07</td>\n",
       "      <td>-1.123596e-07</td>\n",
       "      <td>5.617978e-07</td>\n",
       "      <td>5.617978e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  visit  checkin_bin  exposurebin  age_decade  racecat_combined  \\\n",
       "0        1001      1            2            1           1                 5   \n",
       "1        1002      1            2            1           1                 5   \n",
       "2        1003      1            2            1           1                 5   \n",
       "3        1004      1            1            1           2                 5   \n",
       "4        1005      1            3            0           2                 5   \n",
       "\n",
       "   eduyears  totyr_foot  chiiseas_pf   chiiyrs_pf  ...  explosivity  \\\n",
       "0      16.0    7.000000      4335.40  2167.700000  ...      0.26667   \n",
       "1      15.0   14.000000     10363.10  5708.100000  ...      0.47500   \n",
       "2      18.0   12.000000      6685.40  4863.900000  ...      0.61667   \n",
       "3      16.0   16.000000      7701.20  6448.900000  ...      0.33333   \n",
       "4      21.0   15.852778     10914.89  8229.102222  ...      0.26667   \n",
       "\n",
       "   emo_dyscontrol  impulsivity  affective_lability  nbd_tot  explosivity_z  \\\n",
       "0         0.33333      0.45455             0.20000  31.3636  -1.017540e+00   \n",
       "1         0.75000      0.75758             0.86667  71.2311   3.727100e-01   \n",
       "2         0.91667      0.59848             0.46667  64.9621   1.318090e+00   \n",
       "3         0.41667      0.33333             0.20000  32.0833  -5.726600e-01   \n",
       "4         0.50000      0.31061             0.20000  31.9318   5.056180e-07   \n",
       "\n",
       "   emo_dyscontrol_z  impulsivity_z  affective_lability_z     nbd_tot_z  \n",
       "0     -1.189600e+00  -2.865800e-01         -8.862000e-01 -1.057860e+00  \n",
       "1      8.863700e-01   1.772380e+00          2.782270e+00  1.779890e+00  \n",
       "2      1.716760e+00   6.914300e-01          5.811800e-01  1.333670e+00  \n",
       "3     -7.744100e-01  -1.110160e+00         -8.862000e-01 -1.006630e+00  \n",
       "4      5.056180e-07  -1.123596e-07          5.617978e-07  5.617978e-08  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean for each column, ignoring NaNs\n",
    "mean_values = neuropsych.mean()\n",
    "\n",
    "neuropsych = neuropsych.fillna(mean_values)\n",
    "\n",
    "neuropsych.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51b5dc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id              0\n",
       "visit                   0\n",
       "checkin_bin             0\n",
       "exposurebin             0\n",
       "age_decade              0\n",
       "                       ..\n",
       "explosivity_z           0\n",
       "emo_dyscontrol_z        0\n",
       "impulsivity_z           0\n",
       "affective_lability_z    0\n",
       "nbd_tot_z               0\n",
       "Length: 101, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuropsych_nulls = neuropsych.isnull().sum()\n",
    "neuropsych_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b971c900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bistot</th>\n",
       "      <th>abis_attention</th>\n",
       "      <th>abis_motor</th>\n",
       "      <th>abis_nonplanning</th>\n",
       "      <th>bhstot</th>\n",
       "      <th>bdhi_total</th>\n",
       "      <th>cnstot</th>\n",
       "      <th>bditot</th>\n",
       "      <th>baitot</th>\n",
       "      <th>pcltot</th>\n",
       "      <th>BGLHA_Childhood_Total</th>\n",
       "      <th>BGLHA_Adolescence_Total</th>\n",
       "      <th>BGLHA_Adulthood_Total</th>\n",
       "      <th>nbd_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>31.3636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30</td>\n",
       "      <td>41.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>71.2311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>33</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>64.9621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>32.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>31.9318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bistot  abis_attention  abis_motor  abis_nonplanning  bhstot  bdhi_total  \\\n",
       "0    72.0            12.0         6.0              12.0     0.0        23.0   \n",
       "1    89.0            16.0        11.0              11.0    19.0        35.0   \n",
       "2    82.0            16.0         8.0               9.0     8.0        35.0   \n",
       "3    48.0             6.0         6.0               5.0     6.0        23.0   \n",
       "4    43.0             5.0         4.0               4.0     1.0        23.0   \n",
       "\n",
       "   cnstot  bditot  baitot  pcltot  BGLHA_Childhood_Total  \\\n",
       "0     7.0      14     9.0    19.0                     13   \n",
       "1    17.0      30    41.0    31.0                     20   \n",
       "2    11.0      33    14.0    18.0                     11   \n",
       "3     7.0       4     9.0    17.0                     12   \n",
       "4     7.0       5     1.0     3.0                     11   \n",
       "\n",
       "   BGLHA_Adolescence_Total  BGLHA_Adulthood_Total  nbd_tot  \n",
       "0                       18                     15  31.3636  \n",
       "1                       17                     18  71.2311  \n",
       "2                       12                     14  64.9621  \n",
       "3                       14                     17  32.0833  \n",
       "4                       11                     13  31.9318  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuropsych = neuropsych[['bistot','abis_attention','abis_motor','abis_nonplanning','bhstot','bdhi_total', 'cnstot', 'bditot', 'baitot', 'pcltot', \n",
    "'BGLHA_Childhood_Total', 'BGLHA_Adolescence_Total', 'BGLHA_Adulthood_Total', 'nbd_tot' ]]\n",
    "neuropsych.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c26d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bistot</th>\n",
       "      <th>abis_attention</th>\n",
       "      <th>abis_motor</th>\n",
       "      <th>abis_nonplanning</th>\n",
       "      <th>bhstot</th>\n",
       "      <th>bdhi_total</th>\n",
       "      <th>cnstot</th>\n",
       "      <th>bditot</th>\n",
       "      <th>baitot</th>\n",
       "      <th>pcltot</th>\n",
       "      <th>BGLHA_Childhood_Total</th>\n",
       "      <th>BGLHA_Adolescence_Total</th>\n",
       "      <th>BGLHA_Adulthood_Total</th>\n",
       "      <th>nbd_tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.00000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.00000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>61.931624</td>\n",
       "      <td>10.136752</td>\n",
       "      <td>7.34188</td>\n",
       "      <td>8.538462</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>26.25974</td>\n",
       "      <td>12.038462</td>\n",
       "      <td>10.067797</td>\n",
       "      <td>7.884615</td>\n",
       "      <td>14.888412</td>\n",
       "      <td>12.944915</td>\n",
       "      <td>13.779661</td>\n",
       "      <td>15.521186</td>\n",
       "      <td>42.829579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.042626</td>\n",
       "      <td>3.701422</td>\n",
       "      <td>2.40093</td>\n",
       "      <td>3.098492</td>\n",
       "      <td>4.237623</td>\n",
       "      <td>14.06405</td>\n",
       "      <td>4.293812</td>\n",
       "      <td>9.901841</td>\n",
       "      <td>8.824021</td>\n",
       "      <td>15.671780</td>\n",
       "      <td>3.418525</td>\n",
       "      <td>4.137294</td>\n",
       "      <td>4.625965</td>\n",
       "      <td>13.800428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>27.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.579550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>50.416675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>107.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>65.00000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>87.197000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bistot  abis_attention  abis_motor  abis_nonplanning      bhstot  \\\n",
       "count  236.000000      236.000000   236.00000        236.000000  236.000000   \n",
       "mean    61.931624       10.136752     7.34188          8.538462    3.000000   \n",
       "std     14.042626        3.701422     2.40093          3.098492    4.237623   \n",
       "min     33.000000        5.000000     4.00000          4.000000    0.000000   \n",
       "25%     51.000000        7.000000     5.00000          6.000000    0.000000   \n",
       "50%     61.000000       10.000000     7.00000          8.000000    1.000000   \n",
       "75%     72.000000       13.000000     9.00000         11.000000    4.000000   \n",
       "max    107.000000       20.000000    15.00000         16.000000   19.000000   \n",
       "\n",
       "       bdhi_total      cnstot      bditot      baitot      pcltot  \\\n",
       "count   236.00000  236.000000  236.000000  236.000000  236.000000   \n",
       "mean     26.25974   12.038462   10.067797    7.884615   14.888412   \n",
       "std      14.06405    4.293812    9.901841    8.824021   15.671780   \n",
       "min       3.00000    7.000000    0.000000    0.000000    0.000000   \n",
       "25%      16.00000    8.000000    2.000000    1.000000    3.000000   \n",
       "50%      24.00000   11.000000    8.000000    5.000000   10.000000   \n",
       "75%      35.00000   15.000000   16.000000   12.000000   21.250000   \n",
       "max      65.00000   29.000000   42.000000   42.000000   75.000000   \n",
       "\n",
       "       BGLHA_Childhood_Total  BGLHA_Adolescence_Total  BGLHA_Adulthood_Total  \\\n",
       "count             236.000000               236.000000             236.000000   \n",
       "mean               12.944915                13.779661              15.521186   \n",
       "std                 3.418525                 4.137294               4.625965   \n",
       "min                11.000000                11.000000              11.000000   \n",
       "25%                11.000000                11.000000              12.000000   \n",
       "50%                11.500000                12.000000              14.000000   \n",
       "75%                13.000000                15.000000              18.000000   \n",
       "max                32.000000                34.000000              37.000000   \n",
       "\n",
       "          nbd_tot  \n",
       "count  236.000000  \n",
       "mean    42.829579  \n",
       "std     13.800428  \n",
       "min     27.197000  \n",
       "25%     31.363600  \n",
       "50%     38.579550  \n",
       "75%     50.416675  \n",
       "max     87.197000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuropsych.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e665dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236 entries, 0 to 235\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   bistot                   236 non-null    float64\n",
      " 1   abis_attention           236 non-null    float64\n",
      " 2   abis_motor               236 non-null    float64\n",
      " 3   abis_nonplanning         236 non-null    float64\n",
      " 4   bhstot                   236 non-null    float64\n",
      " 5   bdhi_total               236 non-null    float64\n",
      " 6   cnstot                   236 non-null    float64\n",
      " 7   bditot                   236 non-null    int64  \n",
      " 8   baitot                   236 non-null    float64\n",
      " 9   pcltot                   236 non-null    float64\n",
      " 10  BGLHA_Childhood_Total    236 non-null    int64  \n",
      " 11  BGLHA_Adolescence_Total  236 non-null    int64  \n",
      " 12  BGLHA_Adulthood_Total    236 non-null    int64  \n",
      " 13  nbd_tot                  236 non-null    float64\n",
      "dtypes: float64(10), int64(4)\n",
      "memory usage: 25.9 KB\n"
     ]
    }
   ],
   "source": [
    "neuropsych.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab495d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main_path = r'Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers'\n",
    "folder_name = 'blood_biomk1_to_neuropsych'  \n",
    "save_path = os.path.join(main_path, folder_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7ce67bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bistot  abis_attention  abis_motor  abis_nonplanning  bhstot  bdhi_total  \\\n",
      "0    72.0            12.0         6.0              12.0     0.0        23.0   \n",
      "1    89.0            16.0        11.0              11.0    19.0        35.0   \n",
      "2    82.0            16.0         8.0               9.0     8.0        35.0   \n",
      "3    48.0             6.0         6.0               5.0     6.0        23.0   \n",
      "4    43.0             5.0         4.0               4.0     1.0        23.0   \n",
      "\n",
      "   cnstot  bditot  baitot  pcltot  BGLHA_Childhood_Total  \\\n",
      "0     7.0      14     9.0    19.0                     13   \n",
      "1    17.0      30    41.0    31.0                     20   \n",
      "2    11.0      33    14.0    18.0                     11   \n",
      "3     7.0       4     9.0    17.0                     12   \n",
      "4     7.0       5     1.0     3.0                     11   \n",
      "\n",
      "   BGLHA_Adolescence_Total  BGLHA_Adulthood_Total  nbd_tot  category  \n",
      "0                       18                     15  31.3636         2  \n",
      "1                       17                     18  71.2311         2  \n",
      "2                       12                     14  64.9621         2  \n",
      "3                       14                     17  32.0833         1  \n",
      "4                       11                     13  31.9318         3  \n",
      "   subject_id  visit  checkin_bin  exposurebin  age_decade  racecat_combined  \\\n",
      "0        1001      1            2            1           1                 5   \n",
      "1        1002      1            2            1           1                 5   \n",
      "2        1003      1            2            1           1                 5   \n",
      "3        1004      1            1            1           2                 5   \n",
      "4        1005      1            3            0           2                 5   \n",
      "\n",
      "   eduyears  totyr_foot  chiiseas_pf  chiiyrs_pf  ...  p_IL_7  p_TNF_beta_cv  \\\n",
      "0      16.0         7.0       4335.4      2167.7  ...    3.06          6.040   \n",
      "1      15.0        14.0      10363.1      5708.1  ...   12.30          0.331   \n",
      "2      18.0        12.0       6685.4      4863.9  ...    1.87         15.400   \n",
      "3      16.0        16.0       7701.2      6448.9  ...    9.81          1.860   \n",
      "4      21.0         NaN          NaN         NaN  ...    8.12          0.306   \n",
      "\n",
      "   p_TNF_beta  p_VEGF_A_cv  p_VEGF_A  p_asyn_cv    p_asyn       p_Hb  \\\n",
      "0      0.1210        0.943      16.6       0.51  178000.0   37538.80   \n",
      "1      0.2850        2.640      55.8      12.60  170000.0   26355.70   \n",
      "2      0.8180        5.680      25.3       0.14   79500.0  148871.00   \n",
      "3      0.0472        0.192      62.5       5.33  117000.0   20893.80   \n",
      "4      0.1560        5.370      99.2       4.54  101000.0    8102.65   \n",
      "\n",
      "   dxcte_asyn_p_flag_dv  category  \n",
      "0                   0.0         2  \n",
      "1                   0.0         2  \n",
      "2                   0.0         2  \n",
      "3                   0.0         1  \n",
      "4                   0.0         3  \n",
      "\n",
      "[5 rows x 133 columns]\n"
     ]
    }
   ],
   "source": [
    "categories_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\categories.csv\"\n",
    "categories_df = pd.read_csv(categories_file_path)\n",
    "new_column = categories_df['checkin_bin']\n",
    "neuropsych['category'] = new_column\n",
    "print(neuropsych.head())\n",
    "\n",
    "bloodbiomk2['category'] = new_column\n",
    "print(bloodbiomk2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "103b2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bloodbiomk1['category'] = neuropsych['category']\n",
    "\n",
    "# correlation_matrix = bloodbiomk1.corr()\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "# plt.title('Correlation Heatmap of Features')\n",
    "# plt.show()\n",
    "\n",
    "# # Alternatively, scatter plots for each feature vs y (checkin_bin)\n",
    "# for feature in bloodbiomk1.columns[:-1]:  # Exclude the label column\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(x='category', y=feature, data=bloodbiomk1)\n",
    "#     plt.title(f'Boxplot of {feature} by category')\n",
    "#     plt.xlabel('category')\n",
    "#     plt.ylabel(feature)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44bd65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c6f4a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.5</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.950</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.12</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.2</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.440</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.04</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.220</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.57000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.5</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.000</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.73000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.0</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.200</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>88.2</td>\n",
       "      <td>6.615</td>\n",
       "      <td>31.10</td>\n",
       "      <td>6.515</td>\n",
       "      <td>10722.07</td>\n",
       "      <td>87.30</td>\n",
       "      <td>33.54</td>\n",
       "      <td>3.39000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>76.7</td>\n",
       "      <td>5.490</td>\n",
       "      <td>45.70</td>\n",
       "      <td>9.055</td>\n",
       "      <td>4444.06</td>\n",
       "      <td>4.77</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.37000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>123.0</td>\n",
       "      <td>7.995</td>\n",
       "      <td>37.50</td>\n",
       "      <td>10.650</td>\n",
       "      <td>3908.37</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.66647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>132.0</td>\n",
       "      <td>7.895</td>\n",
       "      <td>63.90</td>\n",
       "      <td>22.700</td>\n",
       "      <td>3582.92</td>\n",
       "      <td>11.23</td>\n",
       "      <td>8.46</td>\n",
       "      <td>1.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>124.0</td>\n",
       "      <td>8.730</td>\n",
       "      <td>44.00</td>\n",
       "      <td>13.150</td>\n",
       "      <td>15650.64</td>\n",
       "      <td>15.50</td>\n",
       "      <td>15.97</td>\n",
       "      <td>4.40000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     p_Ab40  p_Ab42  p_GFAP   p_NfL  p_PDGFRbeta  p_pT181  p_pT231   p_ttau\n",
       "0     126.5   8.535   73.80  11.950      5628.71     8.12     7.58  1.25000\n",
       "1      84.2   7.460   42.05   6.440     10123.09     6.04     7.53  1.20000\n",
       "2     124.0   7.285   34.35   8.220     10045.39     9.33     9.34  1.57000\n",
       "3     110.5   5.920   48.45  13.000      9563.19     6.63     5.87  0.73000\n",
       "4     126.0   9.150   46.75  11.200     12826.15     8.31     7.99  1.54000\n",
       "..      ...     ...     ...     ...          ...      ...      ...      ...\n",
       "231    88.2   6.615   31.10   6.515     10722.07    87.30    33.54  3.39000\n",
       "232    76.7   5.490   45.70   9.055      4444.06     4.77     5.02  2.37000\n",
       "233   123.0   7.995   37.50  10.650      3908.37     3.95     0.90  1.66647\n",
       "234   132.0   7.895   63.90  22.700      3582.92    11.23     8.46  1.08000\n",
       "235   124.0   8.730   44.00  13.150     15650.64    15.50    15.97  4.40000\n",
       "\n",
       "[236 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloodbiomk1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca52a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0180ae7c",
   "metadata": {},
   "source": [
    "\n",
    "# 95th percentile for both analysis and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a17bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in bloodbiomk1.columns[:-1]:  # Assuming 'category' is the last column\n",
    "    upper_limit = bloodbiomk1[feature].quantile(0.95)  # Cap at 95th percentile\n",
    "    lower_limit = bloodbiomk1[feature].quantile(0.05)  # Floor at 5th percentile\n",
    "    bloodbiomk1[feature] = np.where(bloodbiomk1[feature] > upper_limit, upper_limit, bloodbiomk1[feature])\n",
    "    bloodbiomk1[feature] = np.where(bloodbiomk1[feature] < lower_limit, lower_limit, bloodbiomk1[feature])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38819f",
   "metadata": {},
   "source": [
    "# Initial Step for Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a2d78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bloodbiomk1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09f870dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.5</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.950</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.1200</td>\n",
       "      <td>7.5800</td>\n",
       "      <td>1.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.2</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.440</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.0400</td>\n",
       "      <td>7.5300</td>\n",
       "      <td>1.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.220</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.3300</td>\n",
       "      <td>9.3400</td>\n",
       "      <td>1.57000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.5</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.000</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.6300</td>\n",
       "      <td>5.8700</td>\n",
       "      <td>0.73000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.0</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.200</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.3100</td>\n",
       "      <td>7.9900</td>\n",
       "      <td>1.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>88.2</td>\n",
       "      <td>6.615</td>\n",
       "      <td>31.10</td>\n",
       "      <td>6.515</td>\n",
       "      <td>10722.07</td>\n",
       "      <td>22.1075</td>\n",
       "      <td>16.7175</td>\n",
       "      <td>3.39000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>76.7</td>\n",
       "      <td>5.490</td>\n",
       "      <td>45.70</td>\n",
       "      <td>9.055</td>\n",
       "      <td>4444.06</td>\n",
       "      <td>4.7700</td>\n",
       "      <td>5.0200</td>\n",
       "      <td>2.37000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>123.0</td>\n",
       "      <td>7.995</td>\n",
       "      <td>37.50</td>\n",
       "      <td>10.650</td>\n",
       "      <td>3908.37</td>\n",
       "      <td>4.0625</td>\n",
       "      <td>3.1225</td>\n",
       "      <td>1.66647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>132.0</td>\n",
       "      <td>7.895</td>\n",
       "      <td>63.90</td>\n",
       "      <td>22.700</td>\n",
       "      <td>3582.92</td>\n",
       "      <td>11.2300</td>\n",
       "      <td>8.4600</td>\n",
       "      <td>1.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>124.0</td>\n",
       "      <td>8.730</td>\n",
       "      <td>44.00</td>\n",
       "      <td>13.150</td>\n",
       "      <td>15650.64</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>15.9700</td>\n",
       "      <td>4.40000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     p_Ab40  p_Ab42  p_GFAP   p_NfL  p_PDGFRbeta  p_pT181  p_pT231   p_ttau\n",
       "0     126.5   8.535   73.80  11.950      5628.71   8.1200   7.5800  1.25000\n",
       "1      84.2   7.460   42.05   6.440     10123.09   6.0400   7.5300  1.20000\n",
       "2     124.0   7.285   34.35   8.220     10045.39   9.3300   9.3400  1.57000\n",
       "3     110.5   5.920   48.45  13.000      9563.19   6.6300   5.8700  0.73000\n",
       "4     126.0   9.150   46.75  11.200     12826.15   8.3100   7.9900  1.54000\n",
       "..      ...     ...     ...     ...          ...      ...      ...      ...\n",
       "231    88.2   6.615   31.10   6.515     10722.07  22.1075  16.7175  3.39000\n",
       "232    76.7   5.490   45.70   9.055      4444.06   4.7700   5.0200  2.37000\n",
       "233   123.0   7.995   37.50  10.650      3908.37   4.0625   3.1225  1.66647\n",
       "234   132.0   7.895   63.90  22.700      3582.92  11.2300   8.4600  1.08000\n",
       "235   124.0   8.730   44.00  13.150     15650.64  15.5000  15.9700  4.40000\n",
       "\n",
       "[236 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08314cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# got error due to non numeric values, so removing them:\n",
    "\n",
    "# Assume bloodbiomk1 is your DataFrame loaded with various types of data\n",
    "X = bloodbiomk1.copy()\n",
    "\n",
    "# Select only numeric columns from the DataFrame\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "X = X_numeric\n",
    "\n",
    "# Now X_numeric contains only the numeric columns of the original DataFrame\n",
    "# Proceed with scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd00cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns you want to select for modeling\n",
    "\n",
    "# selected_columns = ['bistot','abis_attention','abis_motor','abis_nonplanning','bhstot','bdhi_total', 'cnstot', 'bditot', 'baitot', 'pcltot', \n",
    "# 'BGLHA_Childhood_Total', 'BGLHA_Adolescence_Total', 'BGLHA_Adulthood_Total', 'nbd_tot' ]\n",
    "\n",
    "selected_columns = ['nbd_tot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07f9aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nbd_tot']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70beb6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to feature_importance_rankings.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Random Forest\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# First, calculate feature importances for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train full model to get feature importances\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importance and rank features\n",
    "    feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Save performance metrics with all features as a baseline\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to feature_importance_rankings.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ef49486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Multiple Linear Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for linear regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb96033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 402\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 50\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 99\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 149\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 200\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 250\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 301\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 352\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 402\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 43.191690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Gradient Boosting Machines\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = LGBMRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b43ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"XGBoost\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = xgb.XGBRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = xgb.XGBRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d010d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Support Vector Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for SVR\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train SVR model with all features as a baseline\n",
    "    model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28cba56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 998us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 998us/step\n",
      "3/3 [==============================] - 0s 997us/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002010BB13130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 983us/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002010DE3CAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 998us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 986us/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"ANN\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for ANN\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train ANN model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=X_train_selected.shape[1]),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train_selected, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test_selected).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Scale selected features for ANN\n",
    "        X_train_selected_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_selected_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        # Train and evaluate ANN model with selected features\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_dim=X_train_selected_scaled.shape[1]),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train_selected_scaled, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "        \n",
    "        y_pred = model.predict(X_test_selected_scaled).flatten()\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bd72f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"KNeighbors Regressor\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for KNN\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train KNN model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate KNN model with selected features\n",
    "        model = KNeighborsRegressor(n_neighbors=5)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "294b6b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Stacking Regressor\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base models and final estimator for stacking\n",
    "base_models = [\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=13, leaf_size=23, p=1, weights='uniform')),\n",
    "    ('svr', SVR(kernel='rbf', C=0.2, epsilon=0.01))\n",
    "]\n",
    "final_estimator = Ridge()\n",
    "stack_model = StackingRegressor(estimators=base_models, final_estimator=final_estimator, cv=5)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train stacking model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    stack_model.fit(X_train_selected, y_train)\n",
    "    y_pred = stack_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate stacking model with selected features\n",
    "        stack_model.fit(X_train_selected, y_train)\n",
    "        y_pred = stack_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aec7c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bagging with SVR\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base model for Bagging\n",
    "base_model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train Bagging model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "    bagging_model.fit(X_train_selected, y_train)\n",
    "    y_pred = bagging_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate Bagging model with selected features\n",
    "        bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "        bagging_model.fit(X_train_selected, y_train)\n",
    "        y_pred = bagging_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "628de5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bagging with SVR\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base model for Bagging\n",
    "base_model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train Bagging model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "    bagging_model.fit(X_train_selected, y_train)\n",
    "    y_pred = bagging_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate Bagging model with selected features\n",
    "        bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "        bagging_model.fit(X_train_selected, y_train)\n",
    "        y_pred = bagging_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f0979a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Lasso Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for Lasso regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Lasso Regression Model with L1 regularization for feature selection\n",
    "    model = Lasso(alpha=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Select features based on non-zero coefficients\n",
    "    selector = SelectFromModel(model, prefit=True)\n",
    "    selected_features = X_train.columns[selector.get_support()]\n",
    "    \n",
    "    # Feature importance ranking based on absolute Lasso coefficients\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "\n",
    "    # Evaluate baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = Lasso(alpha=0.1)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aeecddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initialize lists and dictionaries to store results and feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Ridge Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for Ridge regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Ridge Regression Model\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use coefficients for feature importance\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Evaluate baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = Ridge(alpha=1.0)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "112d9cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize lists and dictionaries for storing results and feature rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bayesian Ridge\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for consistency\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train full model to get feature importances\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use coefficients for feature importance ranking\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = BayesianRidge()\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0464b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Gaussian Process Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the kernel\n",
    "kernel = RBF(1.0)\n",
    "\n",
    "# Standardize features for Gaussian Process Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = neuropsych[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train the full Gaussian Process model as a baseline\n",
    "    model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = neuropsych[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b09f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
