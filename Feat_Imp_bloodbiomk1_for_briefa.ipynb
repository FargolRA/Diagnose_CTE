{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af7ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c94958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling biomarkers\n",
    "bloodbiomk1_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\bloodbiomk1_20231117.csv\"\n",
    "bloodbiomk1 = pd.read_csv(bloodbiomk1_file_path)\n",
    "bloodbiomk1 = pd.DataFrame(bloodbiomk1)\n",
    "\n",
    "bloodbiomk2_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\bloodbiomk2_20231117.csv\"\n",
    "bloodbiomk2 = pd.read_csv(bloodbiomk2_file_path)\n",
    "bloodbiomk2 = pd.DataFrame(bloodbiomk2)\n",
    "\n",
    "csfbiomk1_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk1_20231117.csv\"\n",
    "csfbiomk1 = pd.read_csv(csfbiomk1_file_path)\n",
    "csfbiomk1 = pd.DataFrame(csfbiomk1)\n",
    "\n",
    "csfbiomk2_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk2_20231117.csv\"\n",
    "csfbiomk2 = pd.read_csv(csfbiomk2_file_path)\n",
    "csfbiomk2 = pd.DataFrame(csfbiomk2)\n",
    "\n",
    "csfbiomk3_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\csfbiomk3_20231117.csv\"\n",
    "csfbiomk3 = pd.read_csv(csfbiomk3_file_path)\n",
    "csfbiomk3 = pd.DataFrame(csfbiomk3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae0e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main_path = r'Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers'\n",
    "folder_name = 'blood_biomk1_to_briefa'  \n",
    "save_path = os.path.join(main_path, folder_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# save_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\blood_biomk1_briefa\" # Update this path to your desired folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c104242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in bloodbiomk1:\n",
      "                           Column  Number of Nulls\n",
      "0                      subject_id                0\n",
      "1                           visit                0\n",
      "2                     checkin_bin                0\n",
      "3                     exposurebin                0\n",
      "4                      age_decade                0\n",
      "5                racecat_combined                0\n",
      "6                        eduyears                0\n",
      "7                      totyr_foot               56\n",
      "8                     chiiseas_pf               56\n",
      "9                      chiiyrs_pf               56\n",
      "10                    chiiseas_pl               56\n",
      "11                     chiiyrs_pl               56\n",
      "12                    chiiseas_pg               56\n",
      "13                     chiiyrs_pg               56\n",
      "14                         p_Ab40                4\n",
      "15                         p_Ab42                5\n",
      "16                         p_GFAP                8\n",
      "17                          p_NfL                4\n",
      "18    p_Ab40_FLAG_below_ref_range                4\n",
      "19    p_Ab42_FLAG_below_ref_range                4\n",
      "20    p_GFAP_FLAG_below_ref_range                4\n",
      "21     p_NfL_FLAG_below_ref_range                4\n",
      "22                    p_PDGFRbeta                5\n",
      "23         p_PDGFRbeta_FLAG_other              225\n",
      "24                        p_pT181                4\n",
      "25                        p_pT231                4\n",
      "26                         p_ttau               10\n",
      "27  dxcte_totaltau_plasma_flag_dv               10\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "bloodbiomk1_nulls = bloodbiomk1.isnull().sum()\n",
    "\n",
    "bloodbiomk1_nulls_df = pd.DataFrame({\n",
    "    'Column': bloodbiomk1_nulls.index,\n",
    "    'Number of Nulls': bloodbiomk1_nulls.values\n",
    "})\n",
    "\n",
    "bloodbiomk1_nulls_df_transposed = bloodbiomk1_nulls_df.T\n",
    "\n",
    "print(\"Null values in bloodbiomk1:\")\n",
    "print(bloodbiomk1_nulls_df)\n",
    "bloodbiomk1_nulls_df_transposed.to_csv(r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\bloodbiomk1_nulls.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f107621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_Ab40_FLAG_below_ref_range</th>\n",
       "      <th>p_Ab42_FLAG_below_ref_range</th>\n",
       "      <th>p_GFAP_FLAG_below_ref_range</th>\n",
       "      <th>p_NfL_FLAG_below_ref_range</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "      <th>dxcte_totaltau_plasma_flag_dv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.5</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.12</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.2</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.04</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.5</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.0</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Ab40  p_Ab42  p_GFAP  p_NfL  p_Ab40_FLAG_below_ref_range  \\\n",
       "0   126.5   8.535   73.80  11.95                          0.0   \n",
       "1    84.2   7.460   42.05   6.44                          0.0   \n",
       "2   124.0   7.285   34.35   8.22                          0.0   \n",
       "3   110.5   5.920   48.45  13.00                          0.0   \n",
       "4   126.0   9.150   46.75  11.20                          0.0   \n",
       "\n",
       "   p_Ab42_FLAG_below_ref_range  p_GFAP_FLAG_below_ref_range  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   p_NfL_FLAG_below_ref_range  p_PDGFRbeta  p_pT181  p_pT231  p_ttau  \\\n",
       "0                         0.0      5628.71     8.12     7.58    1.25   \n",
       "1                         0.0     10123.09     6.04     7.53    1.20   \n",
       "2                         0.0     10045.39     9.33     9.34    1.57   \n",
       "3                         0.0      9563.19     6.63     5.87    0.73   \n",
       "4                         0.0     12826.15     8.31     7.99    1.54   \n",
       "\n",
       "   dxcte_totaltau_plasma_flag_dv  \n",
       "0                            1.0  \n",
       "1                            1.0  \n",
       "2                            1.0  \n",
       "3                            1.0  \n",
       "4                            1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloodbiomk1 = bloodbiomk1.drop(columns=bloodbiomk1.loc[:,'subject_id':'chiiyrs_pg'].columns)\n",
    "bloodbiomk1 = bloodbiomk1.drop(columns='p_PDGFRbeta_FLAG_other')\n",
    "\n",
    "bloodbiomk1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045b72a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236 entries, 0 to 235\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   p_Ab40       232 non-null    float64\n",
      " 1   p_Ab42       231 non-null    float64\n",
      " 2   p_GFAP       228 non-null    float64\n",
      " 3   p_NfL        232 non-null    float64\n",
      " 4   p_PDGFRbeta  231 non-null    float64\n",
      " 5   p_pT181      232 non-null    float64\n",
      " 6   p_pT231      232 non-null    float64\n",
      " 7   p_ttau       226 non-null    float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 14.9 KB\n"
     ]
    }
   ],
   "source": [
    "bloodbiomk1 = bloodbiomk1.drop(columns=['p_Ab40_FLAG_below_ref_range',\n",
    "                              'p_Ab42_FLAG_below_ref_range',\n",
    "                              'p_GFAP_FLAG_below_ref_range', \n",
    "                              'p_NfL_FLAG_below_ref_range',\n",
    "                              'dxcte_totaltau_plasma_flag_dv'])\n",
    "bloodbiomk1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d355d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.50</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.950</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.12</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.20</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.440</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.04</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.00</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.220</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.50</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.000</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.00</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.200</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.60</td>\n",
       "      <td>3.565</td>\n",
       "      <td>20.80</td>\n",
       "      <td>50.300</td>\n",
       "      <td>4625.97</td>\n",
       "      <td>6.36</td>\n",
       "      <td>5.98</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69.50</td>\n",
       "      <td>5.985</td>\n",
       "      <td>66.50</td>\n",
       "      <td>4.765</td>\n",
       "      <td>9019.82</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5.69</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>76.85</td>\n",
       "      <td>5.465</td>\n",
       "      <td>98.10</td>\n",
       "      <td>9.330</td>\n",
       "      <td>4259.99</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>131.50</td>\n",
       "      <td>9.860</td>\n",
       "      <td>73.70</td>\n",
       "      <td>15.800</td>\n",
       "      <td>7806.22</td>\n",
       "      <td>6.47</td>\n",
       "      <td>3.74</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85.00</td>\n",
       "      <td>7.900</td>\n",
       "      <td>21.70</td>\n",
       "      <td>7.975</td>\n",
       "      <td>6606.37</td>\n",
       "      <td>8.53</td>\n",
       "      <td>6.15</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Ab40  p_Ab42  p_GFAP   p_NfL  p_PDGFRbeta  p_pT181  p_pT231  p_ttau\n",
       "0  126.50   8.535   73.80  11.950      5628.71     8.12     7.58    1.25\n",
       "1   84.20   7.460   42.05   6.440     10123.09     6.04     7.53    1.20\n",
       "2  124.00   7.285   34.35   8.220     10045.39     9.33     9.34    1.57\n",
       "3  110.50   5.920   48.45  13.000      9563.19     6.63     5.87    0.73\n",
       "4  126.00   9.150   46.75  11.200     12826.15     8.31     7.99    1.54\n",
       "5   42.60   3.565   20.80  50.300      4625.97     6.36     5.98    1.74\n",
       "6   69.50   5.985   66.50   4.765      9019.82     5.30     5.69    1.33\n",
       "7   76.85   5.465   98.10   9.330      4259.99     5.87     3.78    1.10\n",
       "8  131.50   9.860   73.70  15.800      7806.22     6.47     3.74    2.14\n",
       "9   85.00   7.900   21.70   7.975      6606.37     8.53     6.15    1.38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloodbiomk1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a38c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.5</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.95</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.12</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.2</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.44</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.04</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.22</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.5</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.00</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.0</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.20</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Ab40  p_Ab42  p_GFAP  p_NfL  p_PDGFRbeta  p_pT181  p_pT231  p_ttau\n",
       "0   126.5   8.535   73.80  11.95      5628.71     8.12     7.58    1.25\n",
       "1    84.2   7.460   42.05   6.44     10123.09     6.04     7.53    1.20\n",
       "2   124.0   7.285   34.35   8.22     10045.39     9.33     9.34    1.57\n",
       "3   110.5   5.920   48.45  13.00      9563.19     6.63     5.87    0.73\n",
       "4   126.0   9.150   46.75  11.20     12826.15     8.31     7.99    1.54"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean for each column, ignoring NaNs\n",
    "mean_values = bloodbiomk1.mean()\n",
    "\n",
    "bloodbiomk1 = bloodbiomk1.fillna(mean_values)\n",
    "\n",
    "bloodbiomk1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5572bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling Neuropsychiatric Measurements\n",
    "\n",
    "briefa_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\briefa_20231117.csv\"\n",
    "briefa = pd.read_csv(briefa_file_path)\n",
    "briefa = pd.DataFrame(briefa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90bdf10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in briefa:\n",
      "          Column  Number of Nulls\n",
      "0     subject_id                0\n",
      "1          visit                0\n",
      "2    checkin_bin                0\n",
      "3    exposurebin                0\n",
      "4     age_decade                0\n",
      "..           ...              ...\n",
      "102      workorg                0\n",
      "103    upseteasy                0\n",
      "104    impulsive                0\n",
      "105       pickup                0\n",
      "106     complete                0\n",
      "\n",
      "[107 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "briefa_nulls = briefa.isnull().sum()\n",
    "\n",
    "briefa_nulls_df = pd.DataFrame({\n",
    "    'Column': briefa_nulls.index,\n",
    "    'Number of Nulls': briefa_nulls.values\n",
    "})\n",
    "\n",
    "briefa_nulls_df_transposed = briefa_nulls_df.T\n",
    "\n",
    "print(\"Null values in briefa:\")\n",
    "print(briefa_nulls_df)\n",
    "briefa_nulls_df_transposed.to_csv(r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\briefa_nulls.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84cb4ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>visit</th>\n",
       "      <th>checkin_bin</th>\n",
       "      <th>exposurebin</th>\n",
       "      <th>age_decade</th>\n",
       "      <th>racecat_combined</th>\n",
       "      <th>eduyears</th>\n",
       "      <th>totyr_foot</th>\n",
       "      <th>chiiseas_pf</th>\n",
       "      <th>chiiyrs_pf</th>\n",
       "      <th>...</th>\n",
       "      <th>activityorg</th>\n",
       "      <th>getover</th>\n",
       "      <th>onething</th>\n",
       "      <th>moodchange</th>\n",
       "      <th>consequence</th>\n",
       "      <th>workorg</th>\n",
       "      <th>upseteasy</th>\n",
       "      <th>impulsive</th>\n",
       "      <th>pickup</th>\n",
       "      <th>complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4335.4</td>\n",
       "      <td>2167.7</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10363.1</td>\n",
       "      <td>5708.1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6685.4</td>\n",
       "      <td>4863.9</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7701.2</td>\n",
       "      <td>6448.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  visit  checkin_bin  exposurebin  age_decade  racecat_combined  \\\n",
       "0        1001      1            2            1           1                 5   \n",
       "1        1002      1            2            1           1                 5   \n",
       "2        1003      1            2            1           1                 5   \n",
       "3        1004      1            1            1           2                 5   \n",
       "4        1005      1            3            0           2                 5   \n",
       "\n",
       "   eduyears  totyr_foot  chiiseas_pf  chiiyrs_pf  ...  activityorg  getover  \\\n",
       "0      16.0         7.0       4335.4      2167.7  ...            2        1   \n",
       "1      15.0        14.0      10363.1      5708.1  ...            3        3   \n",
       "2      18.0        12.0       6685.4      4863.9  ...            3        3   \n",
       "3      16.0        16.0       7701.2      6448.9  ...            1        1   \n",
       "4      21.0         NaN          NaN         NaN  ...            1        2   \n",
       "\n",
       "   onething  moodchange  consequence  workorg  upseteasy  impulsive  pickup  \\\n",
       "0         2           1            1        2          2          1       1   \n",
       "1         3           2            3        3          3          3       2   \n",
       "2         3           3            2        3          3          3       1   \n",
       "3         1           1            1        1          2          1       1   \n",
       "4         1           1            1        1          1          1       1   \n",
       "\n",
       "   complete  \n",
       "0         2  \n",
       "1         3  \n",
       "2         2  \n",
       "3         1  \n",
       "4         1  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "briefa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47af536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mi</th>\n",
       "      <th>tmi</th>\n",
       "      <th>gec</th>\n",
       "      <th>tgec</th>\n",
       "      <th>bri</th>\n",
       "      <th>tbri</th>\n",
       "      <th>inhibit</th>\n",
       "      <th>shift</th>\n",
       "      <th>emotcont</th>\n",
       "      <th>selfmon</th>\n",
       "      <th>initiate</th>\n",
       "      <th>workmem</th>\n",
       "      <th>planorg</th>\n",
       "      <th>taskmon</th>\n",
       "      <th>orgmat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>64</td>\n",
       "      <td>114</td>\n",
       "      <td>57</td>\n",
       "      <td>39</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>85</td>\n",
       "      <td>171</td>\n",
       "      <td>85</td>\n",
       "      <td>70</td>\n",
       "      <td>79</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>77</td>\n",
       "      <td>158</td>\n",
       "      <td>79</td>\n",
       "      <td>68</td>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "      <td>81</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>81</td>\n",
       "      <td>41</td>\n",
       "      <td>38</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mi  tmi  gec  tgec  bri  tbri  inhibit  shift  emotcont  selfmon  \\\n",
       "0   75   64  114    57   39    46       12      9        11        7   \n",
       "1  101   85  171    85   70    79       17     15        23       15   \n",
       "2   90   77  158    79   68    77       15     14        27       12   \n",
       "3   43   40   81    42   38    46        9      8        15        6   \n",
       "4   43   39   81    41   38    45        9     11        12        6   \n",
       "\n",
       "   initiate  workmem  planorg  taskmon  orgmat  \n",
       "0        14       17       20       13      11  \n",
       "1        19       24       27       15      16  \n",
       "2        20       22       22       14      12  \n",
       "3         9        8       11        7       8  \n",
       "4        10        8       10        7       8  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "briefa = briefa.drop(columns=briefa.loc[:,'subject_id':'chiiyrs_pg'].columns)\n",
    "briefa = briefa.drop(columns=briefa.loc[:,'negativ':'complete'].columns)\n",
    "briefa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b971c900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mi</th>\n",
       "      <th>tmi</th>\n",
       "      <th>gec</th>\n",
       "      <th>tgec</th>\n",
       "      <th>bri</th>\n",
       "      <th>tbri</th>\n",
       "      <th>inhibit</th>\n",
       "      <th>shift</th>\n",
       "      <th>emotcont</th>\n",
       "      <th>selfmon</th>\n",
       "      <th>initiate</th>\n",
       "      <th>workmem</th>\n",
       "      <th>planorg</th>\n",
       "      <th>taskmon</th>\n",
       "      <th>orgmat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>236.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63.682203</td>\n",
       "      <td>56.724576</td>\n",
       "      <td>110.169492</td>\n",
       "      <td>56.508475</td>\n",
       "      <td>46.487288</td>\n",
       "      <td>55.203390</td>\n",
       "      <td>12.169492</td>\n",
       "      <td>9.690678</td>\n",
       "      <td>15.758475</td>\n",
       "      <td>8.868644</td>\n",
       "      <td>12.605932</td>\n",
       "      <td>13.758475</td>\n",
       "      <td>15.338983</td>\n",
       "      <td>9.466102</td>\n",
       "      <td>12.512712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.879120</td>\n",
       "      <td>16.771696</td>\n",
       "      <td>33.473344</td>\n",
       "      <td>17.217382</td>\n",
       "      <td>14.657067</td>\n",
       "      <td>16.438698</td>\n",
       "      <td>3.606861</td>\n",
       "      <td>3.378543</td>\n",
       "      <td>5.769042</td>\n",
       "      <td>3.063115</td>\n",
       "      <td>4.101039</td>\n",
       "      <td>4.922909</td>\n",
       "      <td>5.199379</td>\n",
       "      <td>2.946855</td>\n",
       "      <td>4.335387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>79.750000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>104.500000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>79.250000</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>67.250000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>116.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mi         tmi         gec        tgec         bri        tbri  \\\n",
       "count  236.000000  236.000000  236.000000  236.000000  236.000000  236.000000   \n",
       "mean    63.682203   56.724576  110.169492   56.508475   46.487288   55.203390   \n",
       "std     19.879120   16.771696   33.473344   17.217382   14.657067   16.438698   \n",
       "min     40.000000   36.000000   70.000000   35.000000   30.000000   36.000000   \n",
       "25%     45.000000   41.000000   79.750000   41.000000   34.000000   41.000000   \n",
       "50%     60.000000   53.000000  104.500000   54.000000   43.000000   51.000000   \n",
       "75%     79.250000   68.250000  136.000000   69.000000   57.000000   67.250000   \n",
       "max    116.000000  104.000000  204.000000  108.000000   88.000000  105.000000   \n",
       "\n",
       "          inhibit       shift    emotcont     selfmon    initiate     workmem  \\\n",
       "count  236.000000  236.000000  236.000000  236.000000  236.000000  236.000000   \n",
       "mean    12.169492    9.690678   15.758475    8.868644   12.605932   13.758475   \n",
       "std      3.606861    3.378543    5.769042    3.063115    4.101039    4.922909   \n",
       "min      8.000000    6.000000   10.000000    6.000000    8.000000    8.000000   \n",
       "25%      9.000000    6.000000   10.000000    6.000000    9.000000    9.000000   \n",
       "50%     11.000000    9.000000   14.000000    8.000000   12.000000   13.000000   \n",
       "75%     14.000000   12.000000   20.000000   11.000000   16.000000   18.000000   \n",
       "max     23.000000   18.000000   30.000000   18.000000   23.000000   24.000000   \n",
       "\n",
       "          planorg     taskmon      orgmat  \n",
       "count  236.000000  236.000000  236.000000  \n",
       "mean    15.338983    9.466102   12.512712  \n",
       "std      5.199379    2.946855    4.335387  \n",
       "min     10.000000    6.000000    8.000000  \n",
       "25%     10.000000    6.000000    9.000000  \n",
       "50%     14.000000    9.000000   11.000000  \n",
       "75%     19.250000   12.000000   15.000000  \n",
       "max     29.000000   18.000000   24.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "briefa.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c26d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236 entries, 0 to 235\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   p_Ab40       236 non-null    float64\n",
      " 1   p_Ab42       236 non-null    float64\n",
      " 2   p_GFAP       236 non-null    float64\n",
      " 3   p_NfL        236 non-null    float64\n",
      " 4   p_PDGFRbeta  236 non-null    float64\n",
      " 5   p_pT181      236 non-null    float64\n",
      " 6   p_pT231      236 non-null    float64\n",
      " 7   p_ttau       236 non-null    float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 14.9 KB\n"
     ]
    }
   ],
   "source": [
    "bloodbiomk1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab495d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main_path = r'Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers'\n",
    "folder_name = 'blood_biomk1_to_briefa'  \n",
    "save_path = os.path.join(main_path, folder_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ce67bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mi  tmi  gec  tgec  bri  tbri  inhibit  shift  emotcont  selfmon  \\\n",
      "0   75   64  114    57   39    46       12      9        11        7   \n",
      "1  101   85  171    85   70    79       17     15        23       15   \n",
      "2   90   77  158    79   68    77       15     14        27       12   \n",
      "3   43   40   81    42   38    46        9      8        15        6   \n",
      "4   43   39   81    41   38    45        9     11        12        6   \n",
      "\n",
      "   initiate  workmem  planorg  taskmon  orgmat  category  \n",
      "0        14       17       20       13      11         2  \n",
      "1        19       24       27       15      16         2  \n",
      "2        20       22       22       14      12         2  \n",
      "3         9        8       11        7       8         1  \n",
      "4        10        8       10        7       8         3  \n",
      "   subject_id  visit  checkin_bin  exposurebin  age_decade  racecat_combined  \\\n",
      "0        1001      1            2            1           1                 5   \n",
      "1        1002      1            2            1           1                 5   \n",
      "2        1003      1            2            1           1                 5   \n",
      "3        1004      1            1            1           2                 5   \n",
      "4        1005      1            3            0           2                 5   \n",
      "\n",
      "   eduyears  totyr_foot  chiiseas_pf  chiiyrs_pf  ...  p_IL_7  p_TNF_beta_cv  \\\n",
      "0      16.0         7.0       4335.4      2167.7  ...    3.06          6.040   \n",
      "1      15.0        14.0      10363.1      5708.1  ...   12.30          0.331   \n",
      "2      18.0        12.0       6685.4      4863.9  ...    1.87         15.400   \n",
      "3      16.0        16.0       7701.2      6448.9  ...    9.81          1.860   \n",
      "4      21.0         NaN          NaN         NaN  ...    8.12          0.306   \n",
      "\n",
      "   p_TNF_beta  p_VEGF_A_cv  p_VEGF_A  p_asyn_cv    p_asyn       p_Hb  \\\n",
      "0      0.1210        0.943      16.6       0.51  178000.0   37538.80   \n",
      "1      0.2850        2.640      55.8      12.60  170000.0   26355.70   \n",
      "2      0.8180        5.680      25.3       0.14   79500.0  148871.00   \n",
      "3      0.0472        0.192      62.5       5.33  117000.0   20893.80   \n",
      "4      0.1560        5.370      99.2       4.54  101000.0    8102.65   \n",
      "\n",
      "   dxcte_asyn_p_flag_dv  category  \n",
      "0                   0.0         2  \n",
      "1                   0.0         2  \n",
      "2                   0.0         2  \n",
      "3                   0.0         1  \n",
      "4                   0.0         3  \n",
      "\n",
      "[5 rows x 133 columns]\n"
     ]
    }
   ],
   "source": [
    "categories_file_path = r\"Z:\\Active-Diagnose_CTE\\Fargol_Analysis\\Neuropsychiatric_from_Biomarkers\\categories.csv\"\n",
    "categories_df = pd.read_csv(categories_file_path)\n",
    "new_column = categories_df['checkin_bin']\n",
    "briefa['category'] = new_column\n",
    "print(briefa.head())\n",
    "\n",
    "bloodbiomk2['category'] = new_column\n",
    "print(bloodbiomk2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103b2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bloodbiomk1['category'] = briefa['category']\n",
    "\n",
    "# correlation_matrix = bloodbiomk1.corr()\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "# plt.title('Correlation Heatmap of Features')\n",
    "# plt.show()\n",
    "\n",
    "# # Alternatively, scatter plots for each feature vs y (checkin_bin)\n",
    "# for feature in bloodbiomk1.columns[:-1]:  # Exclude the label column\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(x='category', y=feature, data=bloodbiomk1)\n",
    "#     plt.title(f'Boxplot of {feature} by category')\n",
    "#     plt.xlabel('category')\n",
    "#     plt.ylabel(feature)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44bd65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c6f4a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.5</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.950</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.12</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.2</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.440</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.04</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.220</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.57000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.5</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.000</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.63</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.73000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.0</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.200</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>88.2</td>\n",
       "      <td>6.615</td>\n",
       "      <td>31.10</td>\n",
       "      <td>6.515</td>\n",
       "      <td>10722.07</td>\n",
       "      <td>87.30</td>\n",
       "      <td>33.54</td>\n",
       "      <td>3.39000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>76.7</td>\n",
       "      <td>5.490</td>\n",
       "      <td>45.70</td>\n",
       "      <td>9.055</td>\n",
       "      <td>4444.06</td>\n",
       "      <td>4.77</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.37000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>123.0</td>\n",
       "      <td>7.995</td>\n",
       "      <td>37.50</td>\n",
       "      <td>10.650</td>\n",
       "      <td>3908.37</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.66647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>132.0</td>\n",
       "      <td>7.895</td>\n",
       "      <td>63.90</td>\n",
       "      <td>22.700</td>\n",
       "      <td>3582.92</td>\n",
       "      <td>11.23</td>\n",
       "      <td>8.46</td>\n",
       "      <td>1.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>124.0</td>\n",
       "      <td>8.730</td>\n",
       "      <td>44.00</td>\n",
       "      <td>13.150</td>\n",
       "      <td>15650.64</td>\n",
       "      <td>15.50</td>\n",
       "      <td>15.97</td>\n",
       "      <td>4.40000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     p_Ab40  p_Ab42  p_GFAP   p_NfL  p_PDGFRbeta  p_pT181  p_pT231   p_ttau\n",
       "0     126.5   8.535   73.80  11.950      5628.71     8.12     7.58  1.25000\n",
       "1      84.2   7.460   42.05   6.440     10123.09     6.04     7.53  1.20000\n",
       "2     124.0   7.285   34.35   8.220     10045.39     9.33     9.34  1.57000\n",
       "3     110.5   5.920   48.45  13.000      9563.19     6.63     5.87  0.73000\n",
       "4     126.0   9.150   46.75  11.200     12826.15     8.31     7.99  1.54000\n",
       "..      ...     ...     ...     ...          ...      ...      ...      ...\n",
       "231    88.2   6.615   31.10   6.515     10722.07    87.30    33.54  3.39000\n",
       "232    76.7   5.490   45.70   9.055      4444.06     4.77     5.02  2.37000\n",
       "233   123.0   7.995   37.50  10.650      3908.37     3.95     0.90  1.66647\n",
       "234   132.0   7.895   63.90  22.700      3582.92    11.23     8.46  1.08000\n",
       "235   124.0   8.730   44.00  13.150     15650.64    15.50    15.97  4.40000\n",
       "\n",
       "[236 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloodbiomk1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca52a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0180ae7c",
   "metadata": {},
   "source": [
    "\n",
    "# 95th percentile for both analysis and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50a17bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in bloodbiomk1.columns[:-1]:  # Assuming 'category' is the last column\n",
    "    upper_limit = bloodbiomk1[feature].quantile(0.95)  # Cap at 95th percentile\n",
    "    lower_limit = bloodbiomk1[feature].quantile(0.05)  # Floor at 5th percentile\n",
    "    bloodbiomk1[feature] = np.where(bloodbiomk1[feature] > upper_limit, upper_limit, bloodbiomk1[feature])\n",
    "    bloodbiomk1[feature] = np.where(bloodbiomk1[feature] < lower_limit, lower_limit, bloodbiomk1[feature])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38819f",
   "metadata": {},
   "source": [
    "# Initial Step for Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a2d78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bloodbiomk1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f870dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Ab40</th>\n",
       "      <th>p_Ab42</th>\n",
       "      <th>p_GFAP</th>\n",
       "      <th>p_NfL</th>\n",
       "      <th>p_PDGFRbeta</th>\n",
       "      <th>p_pT181</th>\n",
       "      <th>p_pT231</th>\n",
       "      <th>p_ttau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.5</td>\n",
       "      <td>8.535</td>\n",
       "      <td>73.80</td>\n",
       "      <td>11.950</td>\n",
       "      <td>5628.71</td>\n",
       "      <td>8.1200</td>\n",
       "      <td>7.5800</td>\n",
       "      <td>1.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.2</td>\n",
       "      <td>7.460</td>\n",
       "      <td>42.05</td>\n",
       "      <td>6.440</td>\n",
       "      <td>10123.09</td>\n",
       "      <td>6.0400</td>\n",
       "      <td>7.5300</td>\n",
       "      <td>1.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>7.285</td>\n",
       "      <td>34.35</td>\n",
       "      <td>8.220</td>\n",
       "      <td>10045.39</td>\n",
       "      <td>9.3300</td>\n",
       "      <td>9.3400</td>\n",
       "      <td>1.57000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.5</td>\n",
       "      <td>5.920</td>\n",
       "      <td>48.45</td>\n",
       "      <td>13.000</td>\n",
       "      <td>9563.19</td>\n",
       "      <td>6.6300</td>\n",
       "      <td>5.8700</td>\n",
       "      <td>0.73000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126.0</td>\n",
       "      <td>9.150</td>\n",
       "      <td>46.75</td>\n",
       "      <td>11.200</td>\n",
       "      <td>12826.15</td>\n",
       "      <td>8.3100</td>\n",
       "      <td>7.9900</td>\n",
       "      <td>1.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>88.2</td>\n",
       "      <td>6.615</td>\n",
       "      <td>31.10</td>\n",
       "      <td>6.515</td>\n",
       "      <td>10722.07</td>\n",
       "      <td>22.1075</td>\n",
       "      <td>16.7175</td>\n",
       "      <td>3.39000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>76.7</td>\n",
       "      <td>5.490</td>\n",
       "      <td>45.70</td>\n",
       "      <td>9.055</td>\n",
       "      <td>4444.06</td>\n",
       "      <td>4.7700</td>\n",
       "      <td>5.0200</td>\n",
       "      <td>2.37000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>123.0</td>\n",
       "      <td>7.995</td>\n",
       "      <td>37.50</td>\n",
       "      <td>10.650</td>\n",
       "      <td>3908.37</td>\n",
       "      <td>4.0625</td>\n",
       "      <td>3.1225</td>\n",
       "      <td>1.66647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>132.0</td>\n",
       "      <td>7.895</td>\n",
       "      <td>63.90</td>\n",
       "      <td>22.700</td>\n",
       "      <td>3582.92</td>\n",
       "      <td>11.2300</td>\n",
       "      <td>8.4600</td>\n",
       "      <td>1.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>124.0</td>\n",
       "      <td>8.730</td>\n",
       "      <td>44.00</td>\n",
       "      <td>13.150</td>\n",
       "      <td>15650.64</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>15.9700</td>\n",
       "      <td>4.40000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     p_Ab40  p_Ab42  p_GFAP   p_NfL  p_PDGFRbeta  p_pT181  p_pT231   p_ttau\n",
       "0     126.5   8.535   73.80  11.950      5628.71   8.1200   7.5800  1.25000\n",
       "1      84.2   7.460   42.05   6.440     10123.09   6.0400   7.5300  1.20000\n",
       "2     124.0   7.285   34.35   8.220     10045.39   9.3300   9.3400  1.57000\n",
       "3     110.5   5.920   48.45  13.000      9563.19   6.6300   5.8700  0.73000\n",
       "4     126.0   9.150   46.75  11.200     12826.15   8.3100   7.9900  1.54000\n",
       "..      ...     ...     ...     ...          ...      ...      ...      ...\n",
       "231    88.2   6.615   31.10   6.515     10722.07  22.1075  16.7175  3.39000\n",
       "232    76.7   5.490   45.70   9.055      4444.06   4.7700   5.0200  2.37000\n",
       "233   123.0   7.995   37.50  10.650      3908.37   4.0625   3.1225  1.66647\n",
       "234   132.0   7.895   63.90  22.700      3582.92  11.2300   8.4600  1.08000\n",
       "235   124.0   8.730   44.00  13.150     15650.64  15.5000  15.9700  4.40000\n",
       "\n",
       "[236 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08314cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# got error due to non numeric values, so removing them:\n",
    "\n",
    "# Assume bloodbiomk1 is your DataFrame loaded with various types of data\n",
    "X = bloodbiomk1.copy()\n",
    "\n",
    "# Select only numeric columns from the DataFrame\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "X = X_numeric\n",
    "\n",
    "# Now X_numeric contains only the numeric columns of the original DataFrame\n",
    "# Proceed with scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd00cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns you want to select for modeling\n",
    "## columns = ['mi', 'tmi','gec','tgec','bri','tbri','inhibit','shift','emotcont','selfmon','initiate','workmem','planorg','taskmon','orgmat'] \n",
    "selected_columns = ['orgmat']  # Replace 'column1' and 'column2' with the actual column names\n",
    "\n",
    "# selected_columns = ['mi', 'tmi','gec','tgec','bri','tbri','inhibit','shift','emotcont','selfmon','initiate','workmem','planorg','taskmon','orgmat']  # Replace 'column1' and 'column2' with the actual column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07f9aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orgmat']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70beb6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to feature_importance_rankings.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Random Forest\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# First, calculate feature importances for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train full model to get feature importances\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importance and rank features\n",
    "    feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Save performance metrics with all features as a baseline\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to feature_importance_rankings.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ef49486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Multiple Linear Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for linear regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb96033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 402\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 50\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 152\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 202\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 251\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 352\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 402\n",
      "[LightGBM] [Info] Number of data points in the train set: 165, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 12.466667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Gradient Boosting Machines\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = LGBMRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b43ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"XGBoost\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train full model with all features as a baseline\n",
    "    model = xgb.XGBRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = xgb.XGBRegressor(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d010d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Support Vector Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for SVR\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train SVR model with all features as a baseline\n",
    "    model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28cba56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 998us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 982us/step\n",
      "3/3 [==============================] - 0s 997us/step\n",
      "3/3 [==============================] - 0s 998us/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F007B1CAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 997us/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F007B1E200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 982us/step\n",
      "3/3 [==============================] - 0s 986us/step\n",
      "3/3 [==============================] - 0s 997us/step\n",
      "3/3 [==============================] - 0s 984us/step\n",
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"ANN\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for ANN\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train ANN model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=X_train_selected.shape[1]),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train_selected, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test_selected).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Scale selected features for ANN\n",
    "        X_train_selected_scaled = scaler.fit_transform(X_train_selected)\n",
    "        X_test_selected_scaled = scaler.transform(X_test_selected)\n",
    "        \n",
    "        # Train and evaluate ANN model with selected features\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_dim=X_train_selected_scaled.shape[1]),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train_selected_scaled, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "        \n",
    "        y_pred = model.predict(X_test_selected_scaled).flatten()\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bd72f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"KNeighbors Regressor\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for KNN\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train KNN model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate KNN model with selected features\n",
    "        model = KNeighborsRegressor(n_neighbors=5)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "294b6b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Stacking Regressor\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base models and final estimator for stacking\n",
    "base_models = [\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=13, leaf_size=23, p=1, weights='uniform')),\n",
    "    ('svr', SVR(kernel='rbf', C=0.2, epsilon=0.01))\n",
    "]\n",
    "final_estimator = Ridge()\n",
    "stack_model = StackingRegressor(estimators=base_models, final_estimator=final_estimator, cv=5)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train stacking model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    stack_model.fit(X_train_selected, y_train)\n",
    "    y_pred = stack_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate stacking model with selected features\n",
    "        stack_model.fit(X_train_selected, y_train)\n",
    "        y_pred = stack_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aec7c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bagging with SVR\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base model for Bagging\n",
    "base_model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train Bagging model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "    bagging_model.fit(X_train_selected, y_train)\n",
    "    y_pred = bagging_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate Bagging model with selected features\n",
    "        bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "        bagging_model.fit(X_train_selected, y_train)\n",
    "        y_pred = bagging_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "628de5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bagging with SVR\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the base model for Bagging\n",
    "base_model = SVR(kernel='rbf', C=0.2, epsilon=0.01)\n",
    "\n",
    "# Feature selection and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train Bagging model with all features as a baseline\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "    bagging_model.fit(X_train_selected, y_train)\n",
    "    y_pred = bagging_model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate and store baseline performance metrics with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate Bagging model with selected features\n",
    "        bagging_model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, n_jobs=-1)\n",
    "        bagging_model.fit(X_train_selected, y_train)\n",
    "        y_pred = bagging_model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f0979a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Lasso Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for Lasso regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Lasso Regression Model with L1 regularization for feature selection\n",
    "    model = Lasso(alpha=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Select features based on non-zero coefficients\n",
    "    selector = SelectFromModel(model, prefit=True)\n",
    "    selected_features = X_train.columns[selector.get_support()]\n",
    "    \n",
    "    # Feature importance ranking based on absolute Lasso coefficients\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "\n",
    "    # Evaluate baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = Lasso(alpha=0.1)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeecddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initialize lists and dictionaries to store results and feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Ridge Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for Ridge regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Ridge Regression Model\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use coefficients for feature importance\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Evaluate baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = Ridge(alpha=1.0)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "112d9cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize lists and dictionaries for storing results and feature rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Bayesian Ridge\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Standardize features for consistency\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Feature ranking and baseline model training\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train full model to get feature importances\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use coefficients for feature importance ranking\n",
    "    feature_importance = pd.Series(abs(model.coef_), index=X_train.columns)\n",
    "    ranked_features = feature_importance.sort_values(ascending=False)\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Baseline performance with all features\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select top `n_features` based on ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = BayesianRidge()\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0464b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fargor\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance rankings saved to Feature_Importance_rank.xlsx\n",
      "Performance metrics with varying top features saved to performance_with_top_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Initialize an empty list to store results and a dictionary for feature importance rankings\n",
    "results = []\n",
    "feature_importance_rankings = {}\n",
    "model_name = \"Gaussian Process Regression\"\n",
    "selected_columns_str = \"_\".join(selected_columns)\n",
    "\n",
    "# Define the kernel\n",
    "kernel = RBF(1.0)\n",
    "\n",
    "# Standardize features for Gaussian Process Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# First, calculate feature importances using SelectKBest for each target column and save rankings\n",
    "for column in selected_columns:\n",
    "    y = briefa[column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Feature selection using SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "    selector.fit(X_train, y_train)\n",
    "    feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "    ranked_features = feature_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Save the rankings in a dictionary\n",
    "    feature_importance_rankings[column] = ranked_features\n",
    "    \n",
    "    # Train the full Gaussian Process model as a baseline\n",
    "    model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate and store baseline performance with all features\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Column\": column,\n",
    "        \"Top Features\": \"All\",\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R^2 Score\": r2\n",
    "    })\n",
    "\n",
    "# Save feature importance rankings to an Excel file\n",
    "with pd.ExcelWriter(f\"Feature_Importance_bloodbiomk1_rank_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    for column, ranking in feature_importance_rankings.items():\n",
    "        ranking_sorted = ranking.sort_values(ascending=False)\n",
    "        ranking_sorted.to_excel(writer, sheet_name=f\"{model_name}_{column}\")\n",
    "\n",
    "# Test different numbers of top features and evaluate model performance\n",
    "max_features_to_test = X.shape[1]  # Test from 1 up to the total number of features\n",
    "\n",
    "for n_features in range(1, max_features_to_test + 1):\n",
    "    for column in selected_columns:\n",
    "        y = briefa[column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Select the top `n_features` based on initial ranking\n",
    "        top_features = feature_importance_rankings[column].nlargest(n_features).index\n",
    "        X_train_selected = X_train[top_features]\n",
    "        X_test_selected = X_test[top_features]\n",
    "        \n",
    "        # Train and evaluate model with selected features\n",
    "        model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results for each number of top features\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Column\": column,\n",
    "            \"Top Features\": n_features,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            \"Mean Absolute Error\": mae,\n",
    "            \"R^2 Score\": r2\n",
    "        })\n",
    "\n",
    "# Save all results to an Excel file\n",
    "results_df = pd.DataFrame(results)\n",
    "with pd.ExcelWriter(f\"performance_bloodbiomk1_w_top_features_{selected_columns_str}.xlsx\", mode=\"a\", if_sheet_exists=\"new\") as writer:\n",
    "    results_df.to_excel(writer, index=False, sheet_name=model_name)\n",
    "\n",
    "print(\"Feature importance rankings saved to Feature_Importance_rank.xlsx\")\n",
    "print(\"Performance metrics with varying top features saved to performance_with_top_features.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b09f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
